---
title: 'AsyncMultiUnify'
---

```python
class AsyncMultiUnify
```

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L916)</p>



### \_\_init\_\_

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L33)</p>

```python
def __init__(
        self,
        endpoints: Optional[Union[str, Iterable[str]]] = None,
        *,
        system_message: Optional[str] = None,
        messages: Optional[
            Union[
                List[ChatCompletionMessageParam],
                Dict[str, List[ChatCompletionMessageParam]],
            ]
        ] = None,
        frequency_penalty: Optional[float] = None,
        logit_bias: Optional[Dict[str, int]] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        max_completion_tokens: Optional[int] = None,
        n: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        response_format: Optional[Union[Type[BaseModel], Dict[str, str]]] = None,
        seed: Optional[int] = None,
        stop: Union[Optional[str], List[str]] = None,
        temperature: Optional[float] = 1.0,
        top_p: Optional[float] = None,
        tools: Optional[Iterable[ChatCompletionToolParam]] = None,
        tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None,
        parallel_tool_calls: Optional[bool] = None,
        # platform arguments
        use_custom_keys: bool = False,
        tags: Optional[List[str]] = None,
        drop_params: Optional[bool] = True,
        region: Optional[str] = None,
        log_query_body: Optional[bool] = True,
        log_response_body: Optional[bool] = True,
        api_key: Optional[str] = None,
        # python client arguments
        stateful: bool = False,
        return_full_completion: bool = False,
        traced: bool = False,
        cache: Union[bool, str] = None,
        local_cache: bool = True,
        # passthrough arguments
        extra_headers: Optional[Headers] = None,
        extra_query: Optional[Query] = None,
        **kwargs,
    ) -> None:
```

Initialize the Multi LLM Unify client.

**Arguments**:

- `endpoints` - A single endpoint name or a list of endpoint names, with each name
- `system_message` - An optional string containing the system message. This
- `messages` - A list of messages comprising the conversation so far. This will
- `frequency_penalty` - Number between -2.0 and 2.0. Positive values penalize new
- `logit_bias` - Modify the likelihood of specified tokens appearing in the
- `logprobs` - Whether to return log probabilities of the output tokens or not.
- `top_logprobs` - An integer between 0 and 20 specifying the number of most
- `max_completion_tokens` - The maximum number of tokens that can be generated in
- `n` - How many chat completion choices to generate for each input message. Note
- `presence_penalty` - Number between -2.0 and 2.0. Positive values penalize new
- `response_format` - An object specifying the format that the model must output.
- `seed` - If specified, a best effort attempt is made to sample
- `stop` - Up to 4 sequences where the API will stop generating further tokens.
- `temperature` -  What sampling temperature to use, between 0 and 2.
- `top_p` - An alternative to sampling with temperature, called nucleus sampling,
- `tools` - A list of tools the model may call. Currently, only functions are
- `tool_choice` - Controls which (if any) tool is called by the
- `parallel_tool_calls` - Whether to enable parallel function calling during tool
- `use_custom_keys` -  Whether to use custom API keys or our unified API keys
- `tags` - Arbitrary number of tags to classify this API query as needed. Helpful
- `drop_params` - Whether or not to drop unsupported OpenAI params by the
- `region` - A string used to represent the region where the endpoint is
- `log_query_body` - Whether to log the contents of the query json body.
- `log_response_body` - Whether to log the contents of the response json body.
- `stateful` -  Whether the conversation history is preserved within the messages
- `return_full_completion` - If False, only return the message content
- `traced` - Whether to trace the generate method.
- `cache` - If True, then the arguments will be stored in a local cache file, and
- `extra_headers` - Additional "passthrough" headers for the request which are
- `extra_query` - Additional "passthrough" query parameters for the request which
- `kwargs` - Additional "passthrough" JSON properties for the body of the



**Raises**:

- `UnifyError`: If the API key is missing.

## properties

---

### cache

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L441)</p>

```python
def cache(self) -> bool:
```

Get default the cache bool.

**Returns**:

The default cache bool.

---

### clients

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L457)</p>

```python
def clients(self) -> Dict[str, _UniClient]:
```

Get the current dictionary of clients, with endpoint names as keys and
Unify or AsyncUnify instances as values.

**Returns**:

The dictionary of clients.

---

### drop\_params

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L371)</p>

```python
def drop_params(self) -> Optional[bool]:
```

Get the default drop_params bool, if set.

**Returns**:

The default drop_params bool.

---

### endpoints

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L447)</p>

```python
def endpoints(self) -> Tuple[str, ...]:
```

Get the current tuple of endpoints.

**Returns**:

The tuple of endpoints.

---

### extra\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L471)</p>

```python
def extra_body(self) -> Optional[Mapping[str, str]]:
```

Get the default extra body, if set.

**Returns**:

The default extra body.

---

### extra\_headers

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L451)</p>

```python
def extra_headers(self) -> Optional[Headers]:
```

Get the default extra headers, if set.

**Returns**:

The default extra headers.

---

### extra\_query

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L461)</p>

```python
def extra_query(self) -> Optional[Query]:
```

Get the default extra query, if set.

**Returns**:

The default extra query.

---

### frequency\_penalty

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L181)</p>

```python
def frequency_penalty(self) -> Optional[float]:
```

Get the default frequency penalty, if set.

**Returns**:

The default frequency penalty.

---

### input\_cost

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L428)</p>

```python
def input_cost(self) -> Dict[str, float]:
```



---

### itl

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L440)</p>

```python
def itl(self) -> Dict[str, float]:
```



---

### log\_query\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L391)</p>

```python
def log_query_body(self) -> Optional[bool]:
```

Get the default log query body bool, if set.

**Returns**:

The default log query body bool.

---

### log\_response\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L401)</p>

```python
def log_response_body(self) -> Optional[bool]:
```

Get the default log response body bool, if set.

**Returns**:

The default log response body bool.

---

### logit\_bias

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L191)</p>

```python
def logit_bias(self) -> Optional[Dict[str, int]]:
```

Get the default logit bias, if set.

**Returns**:

The default logit bias.

---

### logprobs

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L201)</p>

```python
def logprobs(self) -> Optional[bool]:
```

Get the default logprobs, if set.

**Returns**:

The default logprobs.

---

### max\_completion\_tokens

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L221)</p>

```python
def max_completion_tokens(self) -> Optional[int]:
```

Get the default max tokens, if set.

**Returns**:

The default max tokens.

---

### messages

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L164)</p>

```python
def messages(
        self,
    ) -> Optional[
        Union[
            List[ChatCompletionMessageParam],
            Dict[str, List[ChatCompletionMessageParam]],
        ]
    ]:
```

Get the default messages, if set.

**Returns**:

The default messages.

---

### n

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L231)</p>

```python
def n(self) -> Optional[int]:
```

Get the default n, if set.

**Returns**:

The default n value.

---

### output\_cost

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L432)</p>

```python
def output_cost(self) -> Dict[str, float]:
```



---

### parallel\_tool\_calls

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L341)</p>

```python
def parallel_tool_calls(self) -> Optional[bool]:
```

Get the default parallel tool calls bool, if set.

**Returns**:

The default parallel tool calls bool.

---

### presence\_penalty

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L241)</p>

```python
def presence_penalty(self) -> Optional[float]:
```

Get the default presence penalty, if set.

**Returns**:

The default presence penalty.

---

### region

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L381)</p>

```python
def region(self) -> Optional[str]:
```

Get the default region, if set.

**Returns**:

The default region.

---

### response\_format

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L251)</p>

```python
def response_format(self) -> Optional[Union[Type[BaseModel], Dict[str, str]]]:
```

Get the default response format, if set.

**Returns**:

The default response format.

---

### return\_full\_completion

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L421)</p>

```python
def return_full_completion(self) -> bool:
```

Get the default return full completion bool.

**Returns**:

The default return full completion bool.

---

### seed

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L261)</p>

```python
def seed(self) -> Optional[int]:
```

Get the default seed value, if set.

**Returns**:

The default seed value.

---

### stateful

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L411)</p>

```python
def stateful(self) -> bool:
```

Get the default stateful bool, if set.

**Returns**:

The default stateful bool.

---

### stop

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L271)</p>

```python
def stop(self) -> Union[Optional[str], List[str]]:
```

Get the default stop value, if set.

**Returns**:

The default stop value.

---

### stream

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L281)</p>

```python
def stream(self) -> Optional[bool]:
```

Get the default stream bool, if set.

**Returns**:

The default stream bool.

---

### stream\_options

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L291)</p>

```python
def stream_options(self) -> Optional[ChatCompletionStreamOptionsParam]:
```

Get the default stream options, if set.

**Returns**:

The default stream options.

---

### system\_message

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L154)</p>

```python
def system_message(self) -> Optional[str]:
```

Get the default system message, if set.

**Returns**:

The default system message.

---

### tags

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L361)</p>

```python
def tags(self) -> Optional[List[str]]:
```

Get the default tags, if set.

**Returns**:

The default tags.

---

### temperature

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L301)</p>

```python
def temperature(self) -> Optional[float]:
```

Get the default temperature, if set.

**Returns**:

The default temperature.

---

### tool\_choice

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L331)</p>

```python
def tool_choice(self) -> Optional[ChatCompletionToolChoiceOptionParam]:
```

Get the default tool choice, if set.

**Returns**:

The default tool choice.

---

### tools

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L321)</p>

```python
def tools(self) -> Optional[Iterable[ChatCompletionToolParam]]:
```

Get the default tools, if set.

**Returns**:

The default tools.

---

### top\_logprobs

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L211)</p>

```python
def top_logprobs(self) -> Optional[int]:
```

Get the default top logprobs, if set.

**Returns**:

The default top logprobs.

---

### top\_p

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L311)</p>

```python
def top_p(self) -> Optional[float]:
```

Get the default top p value, if set.

**Returns**:

The default top p value.

---

### traced

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L431)</p>

```python
def traced(self) -> bool:
```

Get the default traced bool.

**Returns**:

The default traced bool.

---

### ttft

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L436)</p>

```python
def ttft(self) -> Dict[str, float]:
```



---

### use\_custom\_keys

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L351)</p>

```python
def use_custom_keys(self) -> bool:
```

Get the default use custom keys bool, if set.

**Returns**:

The default use custom keys bool.

## setters

---

### set\_cache

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L900)</p>

```python
def set_cache(self, value: bool) -> Self:
```

Set the default cache bool.  

**Arguments**:

- `value` - The default cache bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_drop\_params

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L808)</p>

```python
def set_drop_params(self, value: bool) -> Self:
```

Set the default drop params bool.  

**Arguments**:

- `value` - The default drop params bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_extra\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L952)</p>

```python
def set_extra_body(self, value: Body) -> Self:
```

Set the default extra body.  

**Arguments**:

- `value` - The default extra body.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_extra\_headers

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L926)</p>

```python
def set_extra_headers(self, value: Headers) -> Self:
```

Set the default extra headers.  

**Arguments**:

- `value` - The default extra headers.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_extra\_query

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L939)</p>

```python
def set_extra_query(self, value: Query) -> Self:
```

Set the default extra query.  

**Arguments**:

- `value` - The default extra query.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_frequency\_penalty

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L558)</p>

```python
def set_frequency_penalty(self, value: float) -> Self:
```

Set the default frequency penalty.  

**Arguments**:

- `value` - The default frequency penalty.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_local\_cache

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L913)</p>

```python
def set_local_cache(self, value: bool) -> Self:
```

Set the default local cache bool.  

**Arguments**:

- `value` - The default local cache bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_log\_query\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L834)</p>

```python
def set_log_query_body(self, value: bool) -> Self:
```

Set the default log query body bool.  

**Arguments**:

- `value` - The default log query body bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_log\_response\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L847)</p>

```python
def set_log_response_body(self, value: bool) -> Self:
```

Set the default log response body bool.  

**Arguments**:

- `value` - The default log response body bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_logit\_bias

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L571)</p>

```python
def set_logit_bias(self, value: Dict[str, int]) -> Self:
```

Set the default logit bias.  

**Arguments**:

- `value` - The default logit bias.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_logprobs

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L584)</p>

```python
def set_logprobs(self, value: bool) -> Self:
```

Set the default logprobs.  

**Arguments**:

- `value` - The default logprobs.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_max\_completion\_tokens

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L610)</p>

```python
def set_max_completion_tokens(self, value: int) -> Self:
```

Set the default max tokens.  

**Arguments**:

- `value` - The default max tokens.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_messages

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L516)</p>

```python
def set_messages(
        self,
        value: Union[
            List[ChatCompletionMessageParam],
            Dict[str, List[ChatCompletionMessageParam]],
        ],
    ) -> Self:
```

Set the default messages.  

**Arguments**:

- `value` - The default messages.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_n

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L623)</p>

```python
def set_n(self, value: int) -> Self:
```

Set the default n value.  

**Arguments**:

- `value` - The default n value.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_parallel\_tool\_calls

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L769)</p>

```python
def set_parallel_tool_calls(self, value: bool) -> Self:
```

Set the default parallel tool calls bool.  

**Arguments**:

- `value` - The default parallel tool calls bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_presence\_penalty

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L636)</p>

```python
def set_presence_penalty(self, value: float) -> Self:
```

Set the default presence penalty.  

**Arguments**:

- `value` - The default presence penalty.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_region

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L821)</p>

```python
def set_region(self, value: str) -> Self:
```

Set the default region.  

**Arguments**:

- `value` - The default region.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_response\_format

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L649)</p>

```python
def set_response_format(
        self,
        value: Optional[Union[Type[BaseModel], Dict[str, str]]],
    ) -> Self:
```

Set the default response format.  

**Arguments**:

- `value` - The default response format.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_return\_full\_completion

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L873)</p>

```python
def set_return_full_completion(self, value: bool) -> Self:
```

Set the default return full completion bool.  

**Arguments**:

- `value` - The default return full completion bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_seed

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L665)</p>

```python
def set_seed(self, value: Optional[int]) -> Self:
```

Set the default seed value.  

**Arguments**:

- `value` - The default seed value.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_stateful

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L860)</p>

```python
def set_stateful(self, value: bool) -> Self:
```

Set the default stateful bool.  

**Arguments**:

- `value` - The default stateful bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_stop

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L678)</p>

```python
def set_stop(self, value: Union[str, List[str]]) -> Self:
```

Set the default stop value.  

**Arguments**:

- `value` - The default stop value.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_stream

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L691)</p>

```python
def set_stream(self, value: bool) -> Self:
```

Set the default stream bool.  

**Arguments**:

- `value` - The default stream bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_stream\_options

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L704)</p>

```python
def set_stream_options(self, value: ChatCompletionStreamOptionsParam) -> Self:
```

Set the default stream options.  

**Arguments**:

- `value` - The default stream options.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_system\_message

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L484)</p>

```python
def set_system_message(self, value: str) -> Self:
```

Set the default system message.  

**Arguments**:

- `value` - The default system message.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_tags

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L795)</p>

```python
def set_tags(self, value: List[str]) -> Self:
```

Set the default tags.  

**Arguments**:

- `value` - The default tags.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_temperature

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L717)</p>

```python
def set_temperature(self, value: float) -> Self:
```

Set the default temperature.  

**Arguments**:

- `value` - The default temperature.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_tool\_choice

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L756)</p>

```python
def set_tool_choice(self, value: ChatCompletionToolChoiceOptionParam) -> Self:
```

Set the default tool choice.  

**Arguments**:

- `value` - The default tool choice.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_tools

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L743)</p>

```python
def set_tools(self, value: Iterable[ChatCompletionToolParam]) -> Self:
```

Set the default tools.  

**Arguments**:

- `value` - The default tools.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_top\_logprobs

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L597)</p>

```python
def set_top_logprobs(self, value: int) -> Self:
```

Set the default top logprobs.  

**Arguments**:

- `value` - The default top logprobs.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_top\_p

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L730)</p>

```python
def set_top_p(self, value: float) -> Self:
```

Set the default top p value.  

**Arguments**:

- `value` - The default top p value.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_traced

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L887)</p>

```python
def set_traced(self, value: bool) -> Self:
```

Set the default traced bool.  

**Arguments**:

- `value` - The default traced bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_use\_custom\_keys

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L782)</p>

```python
def set_use_custom_keys(self, value: bool) -> Self:
```

Set the default use custom keys bool.  

**Arguments**:

- `value` - The default use custom keys bool.



**Returns**:

This client, useful for chaining inplace calls.

## methods

---

### add\_endpoints

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L316)</p>

```python
def add_endpoints(
        self,
        endpoints: Union[List[str], str],
        ignore_duplicates: bool = True,
    ) -> Self:
```

Add extra endpoints to be queried for each call to generate.

**Arguments**:

- `endpoints` - The extra endpoints to add.
- `ignore_duplicates` - Whether or not to ignore duplicate endpoints passed.



**Returns**:

This client, useful for chaining inplace calls.

---

### append\_messages

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L537)</p>

```python
def append_messages(
        self,
        value: Union[
            List[ChatCompletionMessageParam],
            Dict[str, List[ChatCompletionMessageParam]],
        ],
    ) -> Self:
```

Append to the default messages.  

**Arguments**:

- `value` - The messages to append to the default.



**Returns**:

This client, useful for chaining inplace calls.

---

### copy

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L999)</p>

```python
def copy(self):
```



---

### generate

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L480)</p>

```python
def generate(
        self,
        arg0: Optional[Union[str, List[Union[str, Tuple[Any], Dict[str, Any]]]]] = None,
        /,
        system_message: Optional[str] = None,
        messages: Optional[
            Union[
                List[ChatCompletionMessageParam],
                Dict[str, List[ChatCompletionMessageParam]],
            ]
        ] = None,
        *,
        frequency_penalty: Optional[float] = None,
        logit_bias: Optional[Dict[str, int]] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        max_completion_tokens: Optional[int] = None,
        n: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        response_format: Optional[Union[Type[BaseModel], Dict[str, str]]] = None,
        seed: Optional[int] = None,
        stop: Union[Optional[str], List[str]] = None,
        stream: Optional[bool] = None,
        stream_options: Optional[ChatCompletionStreamOptionsParam] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        tools: Optional[Iterable[ChatCompletionToolParam]] = None,
        tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None,
        parallel_tool_calls: Optional[bool] = None,
        # platform arguments
        use_custom_keys: Optional[bool] = None,
        tags: Optional[List[str]] = None,
        drop_params: Optional[bool] = None,
        region: Optional[str] = None,
        log_query_body: Optional[bool] = None,
        log_response_body: Optional[bool] = None,
        # python client arguments
        stateful: Optional[bool] = None,
        return_full_completion: Optional[bool] = None,
        cache: Optional[Union[bool, str]] = None,
        # passthrough arguments
        extra_headers: Optional[Headers] = None,
        extra_query: Optional[Query] = None,
        **kwargs,
    ):
```

Generate a ChatCompletion response for the specified endpoint,
from the provided query parameters.

**Arguments**:

- `arg0` - A string containing the user message, or a list containing the inputs
- `system_message` - An optional string containing the system message. This
- `messages` - A list of messages comprising the conversation so far, or
- `frequency_penalty` - Number between -2.0 and 2.0. Positive values penalize new
- `logit_bias` - Modify the likelihood of specified tokens appearing in the
- `logprobs` - Whether to return log probabilities of the output tokens or not.
- `top_logprobs` - An integer between 0 and 20 specifying the number of most
- `max_completion_tokens` - The maximum number of tokens that can be generated in
- `n` - How many chat completion choices to generate for each input message. Note
- `presence_penalty` - Number between -2.0 and 2.0. Positive values penalize new
- `response_format` - An object specifying the format that the model must output.
- `seed` - If specified, a best effort attempt is made to sample
- `stop` - Up to 4 sequences where the API will stop generating further tokens.
- `stream` - If True, generates content as a stream. If False, generates content
- `stream_options` - Options for streaming response. Only set this when you set
- `stream` - true.
- `temperature` -  What sampling temperature to use, between 0 and 2.
- `top_p` - An alternative to sampling with temperature, called nucleus sampling,
- `tools` - A list of tools the model may call. Currently, only functions are
- `tool_choice` - Controls which (if any) tool is called by the
- `parallel_tool_calls` - Whether to enable parallel function calling during tool
- `stateful` -  Whether the conversation history is preserved within the messages
- `use_custom_keys` -  Whether to use custom API keys or our unified API keys
- `tags` - Arbitrary number of tags to classify this API query as needed. Helpful
- `drop_params` - Whether or not to drop unsupported OpenAI params by the
- `region` - A string used to represent the region where the endpoint is
- `log_query_body` - Whether to log the contents of the query json body.
- `log_response_body` - Whether to log the contents of the response json body.
- `stateful` -  Whether the conversation history is preserved within the messages
- `return_full_completion` - If False, only return the message content
- `cache` - If True, then the arguments will be stored in a local cache file, and
- `extra_headers` - Additional "passthrough" headers for the request which are
- `extra_query` - Additional "passthrough" query parameters for the request which
- `kwargs` - Additional "passthrough" JSON properties for the body of the



**Returns**:

If stream is True, returns a generator yielding chunks of content.
If stream is False, returns a single string response.



**Raises**:

- `UnifyError`: If an error occurs during content generation.

---

### get\_credit\_balance

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L394)</p>

```python
def get_credit_balance(self) -> Union[float, None]:
```

Get the remaining credits left on your account.

**Returns**:

The remaining credits on the account if successful, otherwise None.
Raises:
BadRequestError: If there was an HTTP error.
ValueError: If there was an error parsing the JSON response.
- `BadRequestError` - If there was an HTTP error.
- `ValueError` - If there was an error parsing the JSON response.

---

### json

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/base.py#L1047)</p>

```python
def json(self):
```



---

### remove\_endpoints

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L354)</p>

```python
def remove_endpoints(
        self,
        endpoints: Union[List[str], str],
        ignore_missing: bool = True,
    ) -> Self:
```

Remove endpoints from the current list, which are queried for each call to
generate.

**Arguments**:

- `endpoints` - The extra endpoints to add.
- `ignore_missing` - Whether or not to ignore endpoints passed which are not



**Returns**:

This client, useful for chaining inplace calls.

---

### to\_sync\_client

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L995)</p>

```python
def to_sync_client(self):
```

Return a synchronous version of the client (`MultiUnify` instance), with the
exact same configuration as this asynchronous (`AsyncMultiUnify`) client.

**Returns**:

A `MultiUnify` instance with the same configuration as this `AsyncMultiUnify`
instance.

## dunder_methods

---

### \_\_repr\_\_

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L471)</p>

```python
def __repr__(self):
```



---

### \_\_str\_\_

<p align="right">[source code](https://github.com/unifyai/unify/tree/338af57153e2c8490e44e7153be6876b975790d9/unify/universal_api/clients/multi_llm.py#L474)</p>

```python
def __str__(self):
```

