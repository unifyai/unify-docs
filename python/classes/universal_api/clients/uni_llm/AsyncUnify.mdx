---
title: 'AsyncUnify'
---

```python
class AsyncUnify
```

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L969)</p>

Class for interacting with the Unify chat completions endpoint in a synchronous
    manner.

## properties

---

### cache

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L419)</p>

```python
def cache(self) -> bool:
```

Get default the cache bool.

**Returns**:

The default cache bool.

---

### default\_prompt

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L459)</p>

```python
def default_prompt(self) -> Prompt:
```

Get the default prompt, if set.

**Returns**:

The default prompt.

---

### drop\_params

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L359)</p>

```python
def drop_params(self) -> Optional[bool]:
```

Get the default drop_params bool, if set.

**Returns**:

The default drop_params bool.

---

### endpoint

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L301)</p>

```python
def endpoint(self) -> str:
```

Get the endpoint name.

**Returns**:

The endpoint name.

---

### extra\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L449)</p>

```python
def extra_body(self) -> Optional[Mapping[str, str]]:
```

Get the default extra body, if set.

**Returns**:

The default extra body.

---

### extra\_headers

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L429)</p>

```python
def extra_headers(self) -> Optional[Headers]:
```

Get the default extra headers, if set.

**Returns**:

The default extra headers.

---

### extra\_query

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L439)</p>

```python
def extra_query(self) -> Optional[Query]:
```

Get the default extra query, if set.

**Returns**:

The default extra query.

---

### frequency\_penalty

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L169)</p>

```python
def frequency_penalty(self) -> Optional[float]:
```

Get the default frequency penalty, if set.

**Returns**:

The default frequency penalty.

---

### input\_cost

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L282)</p>

```python
def input_cost(self) -> float:
```



---

### inter\_token\_latency

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L294)</p>

```python
def inter_token_latency(self) -> float:
```



---

### log\_query\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L379)</p>

```python
def log_query_body(self) -> Optional[bool]:
```

Get the default log query body bool, if set.

**Returns**:

The default log query body bool.

---

### log\_response\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L389)</p>

```python
def log_response_body(self) -> Optional[bool]:
```

Get the default log response body bool, if set.

**Returns**:

The default log response body bool.

---

### logit\_bias

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L179)</p>

```python
def logit_bias(self) -> Optional[Dict[str, int]]:
```

Get the default logit bias, if set.

**Returns**:

The default logit bias.

---

### logprobs

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L189)</p>

```python
def logprobs(self) -> Optional[bool]:
```

Get the default logprobs, if set.

**Returns**:

The default logprobs.

---

### max\_completion\_tokens

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L209)</p>

```python
def max_completion_tokens(self) -> Optional[int]:
```

Get the default max tokens, if set.

**Returns**:

The default max tokens.

---

### messages

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L152)</p>

```python
def messages(
        self,
    ) -> Optional[
        Union[
            List[ChatCompletionMessageParam],
            Dict[str, List[ChatCompletionMessageParam]],
        ]
    ]:
```

Get the default messages, if set.

**Returns**:

The default messages.

---

### model

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L311)</p>

```python
def model(self) -> str:
```

Get the model name.

**Returns**:

The model name.

---

### n

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L219)</p>

```python
def n(self) -> Optional[int]:
```

Get the default n, if set.

**Returns**:

The default n value.

---

### output\_cost

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L286)</p>

```python
def output_cost(self) -> float:
```



---

### parallel\_tool\_calls

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L329)</p>

```python
def parallel_tool_calls(self) -> Optional[bool]:
```

Get the default parallel tool calls bool, if set.

**Returns**:

The default parallel tool calls bool.

---

### presence\_penalty

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L229)</p>

```python
def presence_penalty(self) -> Optional[float]:
```

Get the default presence penalty, if set.

**Returns**:

The default presence penalty.

---

### provider

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L321)</p>

```python
def provider(self) -> str:
```

Get the provider name.

**Returns**:

The provider name.

---

### region

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L369)</p>

```python
def region(self) -> Optional[str]:
```

Get the default region, if set.

**Returns**:

The default region.

---

### response\_format

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L239)</p>

```python
def response_format(self) -> Optional[ResponseFormat]:
```

Get the default response format, if set.

**Returns**:

The default response format.

---

### return\_full\_completion

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L409)</p>

```python
def return_full_completion(self) -> bool:
```

Get the default return full completion bool.

**Returns**:

The default return full completion bool.

---

### seed

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L249)</p>

```python
def seed(self) -> Optional[int]:
```

Get the default seed value, if set.

**Returns**:

The default seed value.

---

### stateful

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L399)</p>

```python
def stateful(self) -> bool:
```

Get the default stateful bool, if set.

**Returns**:

The default stateful bool.

---

### stop

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L259)</p>

```python
def stop(self) -> Union[Optional[str], List[str]]:
```

Get the default stop value, if set.

**Returns**:

The default stop value.

---

### stream

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L269)</p>

```python
def stream(self) -> Optional[bool]:
```

Get the default stream bool, if set.

**Returns**:

The default stream bool.

---

### stream\_options

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L279)</p>

```python
def stream_options(self) -> Optional[ChatCompletionStreamOptionsParam]:
```

Get the default stream options, if set.

**Returns**:

The default stream options.

---

### system\_message

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L142)</p>

```python
def system_message(self) -> Optional[str]:
```

Get the default system message, if set.

**Returns**:

The default system message.

---

### tags

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L349)</p>

```python
def tags(self) -> Optional[List[str]]:
```

Get the default tags, if set.

**Returns**:

The default tags.

---

### temperature

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L289)</p>

```python
def temperature(self) -> Optional[float]:
```

Get the default temperature, if set.

**Returns**:

The default temperature.

---

### time\_to\_first\_token

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L290)</p>

```python
def time_to_first_token(self) -> float:
```



---

### tool\_choice

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L319)</p>

```python
def tool_choice(self) -> Optional[ChatCompletionToolChoiceOptionParam]:
```

Get the default tool choice, if set.

**Returns**:

The default tool choice.

---

### tools

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L309)</p>

```python
def tools(self) -> Optional[Iterable[ChatCompletionToolParam]]:
```

Get the default tools, if set.

**Returns**:

The default tools.

---

### top\_logprobs

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L199)</p>

```python
def top_logprobs(self) -> Optional[int]:
```

Get the default top logprobs, if set.

**Returns**:

The default top logprobs.

---

### top\_p

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L299)</p>

```python
def top_p(self) -> Optional[float]:
```

Get the default top p value, if set.

**Returns**:

The default top p value.

---

### use\_custom\_keys

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L339)</p>

```python
def use_custom_keys(self) -> bool:
```

Get the default use custom keys bool, if set.

**Returns**:

The default use custom keys bool.

## setters

---

### set\_cache

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L854)</p>

```python
def set_cache(self, value: bool) -> Self:
```

Set the default cache bool.  

**Arguments**:

- `value` - The default cache bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_default\_prompt

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L906)</p>

```python
def set_default_prompt(self, value: Prompt) -> Self:
```

Set the default prompt.  

**Returns**:

This client, useful for chaining inplace calls.

---

### set\_drop\_params

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L776)</p>

```python
def set_drop_params(self, value: bool) -> Self:
```

Set the default drop params bool.  

**Arguments**:

- `value` - The default drop params bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_endpoint

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L334)</p>

```python
def set_endpoint(self, value: str) -> Self:
```

Set the endpoint name.  

**Arguments**:

- `value` - The endpoint name.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_extra\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L893)</p>

```python
def set_extra_body(self, value: Body) -> Self:
```

Set the default extra body.  

**Arguments**:

- `value` - The default extra body.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_extra\_headers

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L867)</p>

```python
def set_extra_headers(self, value: Headers) -> Self:
```

Set the default extra headers.  

**Arguments**:

- `value` - The default extra headers.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_extra\_query

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L880)</p>

```python
def set_extra_query(self, value: Query) -> Self:
```

Set the default extra query.  

**Arguments**:

- `value` - The default extra query.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_frequency\_penalty

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L529)</p>

```python
def set_frequency_penalty(self, value: float) -> Self:
```

Set the default frequency penalty.  

**Arguments**:

- `value` - The default frequency penalty.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_log\_query\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L802)</p>

```python
def set_log_query_body(self, value: bool) -> Self:
```

Set the default log query body bool.  

**Arguments**:

- `value` - The default log query body bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_log\_response\_body

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L815)</p>

```python
def set_log_response_body(self, value: bool) -> Self:
```

Set the default log response body bool.  

**Arguments**:

- `value` - The default log response body bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_logit\_bias

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L542)</p>

```python
def set_logit_bias(self, value: Dict[str, int]) -> Self:
```

Set the default logit bias.  

**Arguments**:

- `value` - The default logit bias.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_logprobs

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L555)</p>

```python
def set_logprobs(self, value: bool) -> Self:
```

Set the default logprobs.  

**Arguments**:

- `value` - The default logprobs.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_max\_completion\_tokens

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L581)</p>

```python
def set_max_completion_tokens(self, value: int) -> Self:
```

Set the default max tokens.  

**Arguments**:

- `value` - The default max tokens.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_messages

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L491)</p>

```python
def set_messages(
        self,
        value: Union[
            List[ChatCompletionMessageParam],
            Dict[str, List[ChatCompletionMessageParam]],
        ],
    ) -> Self:
```

Set the default messages.  

**Arguments**:

- `value` - The default messages.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_model

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L356)</p>

```python
def set_model(self, value: str) -> Self:
```

Set the model name.  

**Arguments**:

- `value` - The model name.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_n

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L594)</p>

```python
def set_n(self, value: int) -> Self:
```

Set the default n value.  

**Arguments**:

- `value` - The default n value.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_parallel\_tool\_calls

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L737)</p>

```python
def set_parallel_tool_calls(self, value: bool) -> Self:
```

Set the default parallel tool calls bool.  

**Arguments**:

- `value` - The default parallel tool calls bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_presence\_penalty

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L607)</p>

```python
def set_presence_penalty(self, value: float) -> Self:
```

Set the default presence penalty.  

**Arguments**:

- `value` - The default presence penalty.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_provider

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L388)</p>

```python
def set_provider(self, value: str) -> Self:
```

Set the provider name.  

**Arguments**:

- `value` - The provider name.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_region

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L789)</p>

```python
def set_region(self, value: str) -> Self:
```

Set the default region.  

**Arguments**:

- `value` - The default region.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_response\_format

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L620)</p>

```python
def set_response_format(self, value: ResponseFormat) -> Self:
```

Set the default response format.  

**Arguments**:

- `value` - The default response format.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_return\_full\_completion

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L841)</p>

```python
def set_return_full_completion(self, value: bool) -> Self:
```

Set the default return full completion bool.  

**Arguments**:

- `value` - The default return full completion bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_seed

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L633)</p>

```python
def set_seed(self, value: Optional[int]) -> Self:
```

Set the default seed value.  

**Arguments**:

- `value` - The default seed value.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_stateful

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L828)</p>

```python
def set_stateful(self, value: bool) -> Self:
```

Set the default stateful bool.  

**Arguments**:

- `value` - The default stateful bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_stop

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L646)</p>

```python
def set_stop(self, value: Union[str, List[str]]) -> Self:
```

Set the default stop value.  

**Arguments**:

- `value` - The default stop value.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_stream

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L659)</p>

```python
def set_stream(self, value: bool) -> Self:
```

Set the default stream bool.  

**Arguments**:

- `value` - The default stream bool.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_stream\_options

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L672)</p>

```python
def set_stream_options(self, value: ChatCompletionStreamOptionsParam) -> Self:
```

Set the default stream options.  

**Arguments**:

- `value` - The default stream options.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_system\_message

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L478)</p>

```python
def set_system_message(self, value: str) -> Self:
```

Set the default system message.  

**Arguments**:

- `value` - The default system message.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_tags

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L763)</p>

```python
def set_tags(self, value: List[str]) -> Self:
```

Set the default tags.  

**Arguments**:

- `value` - The default tags.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_temperature

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L685)</p>

```python
def set_temperature(self, value: float) -> Self:
```

Set the default temperature.  

**Arguments**:

- `value` - The default temperature.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_tool\_choice

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L724)</p>

```python
def set_tool_choice(self, value: ChatCompletionToolChoiceOptionParam) -> Self:
```

Set the default tool choice.  

**Arguments**:

- `value` - The default tool choice.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_tools

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L711)</p>

```python
def set_tools(self, value: Iterable[ChatCompletionToolParam]) -> Self:
```

Set the default tools.  

**Arguments**:

- `value` - The default tools.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_top\_logprobs

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L568)</p>

```python
def set_top_logprobs(self, value: int) -> Self:
```

Set the default top logprobs.  

**Arguments**:

- `value` - The default top logprobs.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_top\_p

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L698)</p>

```python
def set_top_p(self, value: float) -> Self:
```

Set the default top p value.  

**Arguments**:

- `value` - The default top p value.



**Returns**:

This client, useful for chaining inplace calls.

---

### set\_use\_custom\_keys

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L750)</p>

```python
def set_use_custom_keys(self, value: bool) -> Self:
```

Set the default use custom keys bool.  

**Arguments**:

- `value` - The default use custom keys bool.



**Returns**:

This client, useful for chaining inplace calls.

## methods

---

### append\_messages

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L510)</p>

```python
def append_messages(
        self,
        value: Union[
            List[ChatCompletionMessageParam],
            Dict[str, List[ChatCompletionMessageParam]],
        ],
    ) -> Self:
```

Append to the default messages.  

**Arguments**:

- `value` - The messages to append to the default.



**Returns**:

This client, useful for chaining inplace calls.

---

### generate

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L477)</p>

```python
def generate(
        self,
        user_message: Optional[str] = None,
        system_message: Optional[str] = None,
        messages: Optional[
            Union[
                List[ChatCompletionMessageParam],
                Dict[str, List[ChatCompletionMessageParam]],
            ]
        ] = None,
        *,
        frequency_penalty: Optional[float] = None,
        logit_bias: Optional[Dict[str, int]] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        max_completion_tokens: Optional[int] = None,
        n: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        response_format: Optional[ResponseFormat] = None,
        seed: Optional[int] = None,
        stop: Union[Optional[str], List[str]] = None,
        stream: Optional[bool] = None,
        stream_options: Optional[ChatCompletionStreamOptionsParam] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        tools: Optional[Iterable[ChatCompletionToolParam]] = None,
        tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None,
        parallel_tool_calls: Optional[bool] = None,
        # platform arguments
        use_custom_keys: Optional[bool] = None,
        tags: Optional[List[str]] = None,
        drop_params: Optional[bool] = None,
        region: Optional[str] = None,
        log_query_body: Optional[bool] = None,
        log_response_body: Optional[bool] = None,
        # python client arguments
        stateful: Optional[bool] = None,
        return_full_completion: Optional[bool] = None,
        cache: Optional[bool] = None,
        # passthrough arguments
        extra_headers: Optional[Headers] = None,
        extra_query: Optional[Query] = None,
        **kwargs,
    ):
```

Generate a ChatCompletion response for the specified endpoint,
from the provided query parameters.

**Arguments**:

- `user_message` - A string containing the user message.
- `system_message` - An optional string containing the system message. This
- `messages` - A list of messages comprising the conversation so far, or
- `frequency_penalty` - Number between -2.0 and 2.0. Positive values penalize new
- `logit_bias` - Modify the likelihood of specified tokens appearing in the
- `logprobs` - Whether to return log probabilities of the output tokens or not.
- `top_logprobs` - An integer between 0 and 20 specifying the number of most
- `max_completion_tokens` - The maximum number of tokens that can be generated in
- `n` - How many chat completion choices to generate for each input message. Note
- `presence_penalty` - Number between -2.0 and 2.0. Positive values penalize new
- `response_format` - An object specifying the format that the model must output.
- `seed` - If specified, a best effort attempt is made to sample
- `stop` - Up to 4 sequences where the API will stop generating further tokens.
- `stream` - If True, generates content as a stream. If False, generates content
- `stream_options` - Options for streaming response. Only set this when you set
- `stream` - true.
- `temperature` -  What sampling temperature to use, between 0 and 2.
- `top_p` - An alternative to sampling with temperature, called nucleus sampling,
- `tools` - A list of tools the model may call. Currently, only functions are
- `tool_choice` - Controls which (if any) tool is called by the
- `parallel_tool_calls` - Whether to enable parallel function calling during tool
- `use_custom_keys` -  Whether to use custom API keys or our unified API keys
- `tags` - Arbitrary number of tags to classify this API query as needed. Helpful
- `drop_params` - Whether or not to drop unsupported OpenAI params by the
- `region` - A string used to represent the region where the endpoint is
- `log_query_body` - Whether to log the contents of the query json body.
- `log_response_body` - Whether to log the contents of the response json body.
- `stateful` -  Whether the conversation history is preserved within the messages
- `return_full_completion` - If False, only return the message content
- `cache` - If True, then the arguments will be stored in a local cache file, and
- `extra_headers` - Additional "passthrough" headers for the request which are
- `extra_query` - Additional "passthrough" query parameters for the request which
- `kwargs` - Additional "passthrough" JSON properties for the body of the



**Returns**:

If stream is True, returns a generator yielding chunks of content.
If stream is False, returns a single string response.



**Raises**:

- `UnifyError`: If an error occurs during content generation.

---

### get\_credit\_balance

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/base.py#L924)</p>

```python
def get_credit_balance(self) -> Union[float, None]:
```

Get the remaining credits left on your account.

**Returns**:

The remaining credits on the account if successful, otherwise None.
Raises:
BadRequestError: If there was an HTTP error.
ValueError: If there was an error parsing the JSON response.
- `BadRequestError` - If there was an HTTP error.
- `ValueError` - If there was an error parsing the JSON response.

---

### to\_sync\_client

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L1175)</p>

```python
def to_sync_client(self):
```

Return a synchronous version of the client (`Unify` instance), with the
exact same configuration as this asynchronous (`AsyncUnify`) client.

**Returns**:

A `Unify` instance with the same configuration as this `AsyncUnify`
instance.

## dunder_methods

---

### \_\_init\_\_

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L30)</p>

```python
def __init__(
        self,
        endpoint: Optional[str] = None,
        *,
        model: Optional[str] = None,
        provider: Optional[str] = None,
        system_message: Optional[str] = None,
        messages: Optional[List[ChatCompletionMessageParam]] = None,
        frequency_penalty: Optional[float] = None,
        logit_bias: Optional[Dict[str, int]] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        max_completion_tokens: Optional[int] = 1024,
        n: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        response_format: Optional[ResponseFormat] = None,
        seed: Optional[int] = None,
        stop: Union[Optional[str], List[str]] = None,
        stream: Optional[bool] = False,
        stream_options: Optional[ChatCompletionStreamOptionsParam] = None,
        temperature: Optional[float] = 1.0,
        top_p: Optional[float] = None,
        tools: Optional[Iterable[ChatCompletionToolParam]] = None,
        tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None,
        parallel_tool_calls: Optional[bool] = None,
        # platform arguments
        use_custom_keys: bool = False,
        tags: Optional[List[str]] = None,
        drop_params: Optional[bool] = True,
        region: Optional[str] = None,
        log_query_body: Optional[bool] = True,
        log_response_body: Optional[bool] = True,
        api_key: Optional[str] = None,
        # python client arguments
        stateful: bool = False,
        return_full_completion: bool = False,
        cache: bool = False,
        # passthrough arguments
        extra_headers: Optional[Headers] = None,
        extra_query: Optional[Query] = None,
        **kwargs,
    ):
```

Initialize the Uni LLM Unify client.

**Arguments**:

- `endpoint` - Endpoint name in OpenAI API format:
- `model` - Name of the model. Should only be set if endpoint is not set.
- `provider` - Name of the provider. Should only be set if endpoint is not set.
- `system_message` - An optional string containing the system message. This
- `messages` - A list of messages comprising the conversation so far. This will
- `frequency_penalty` - Number between -2.0 and 2.0. Positive values penalize new
- `logit_bias` - Modify the likelihood of specified tokens appearing in the
- `logprobs` - Whether to return log probabilities of the output tokens or not.
- `top_logprobs` - An integer between 0 and 20 specifying the number of most
- `max_completion_tokens` - The maximum number of tokens that can be generated in
- `n` - How many chat completion choices to generate for each input message. Note
- `presence_penalty` - Number between -2.0 and 2.0. Positive values penalize new
- `response_format` - An object specifying the format that the model must output.
- `seed` - If specified, a best effort attempt is made to sample
- `stop` - Up to 4 sequences where the API will stop generating further tokens.
- `stream` - If True, generates content as a stream. If False, generates content
- `stream_options` - Options for streaming response. Only set this when you set
- `stream` - true.
- `temperature` -  What sampling temperature to use, between 0 and 2.
- `top_p` - An alternative to sampling with temperature, called nucleus sampling,
- `tools` - A list of tools the model may call. Currently, only functions are
- `tool_choice` - Controls which (if any) tool is called by the
- `parallel_tool_calls` - Whether to enable parallel function calling during tool
- `use_custom_keys` -  Whether to use custom API keys or our unified API keys
- `tags` - Arbitrary number of tags to classify this API query as needed. Helpful
- `drop_params` - Whether or not to drop unsupported OpenAI params by the
- `region` - A string used to represent the region where the endpoint is
- `log_query_body` - Whether to log the contents of the query json body.
- `log_response_body` - Whether to log the contents of the response json body.
- `stateful` -  Whether the conversation history is preserved within the messages
- `return_full_completion` - If False, only return the message content
- `cache` - If True, then the arguments will be stored in a local cache file, and
- `extra_headers` - Additional "passthrough" headers for the request which are
- `extra_query` - Additional "passthrough" query parameters for the request which
- `kwargs` - Additional "passthrough" JSON properties for the body of the



**Raises**:

- `UnifyError`: If the API key is missing.

---

### \_\_repr\_\_

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L461)</p>

```python
def __repr__(self):
```



---

### \_\_str\_\_

<p align="right">[source code](https://github.com/unifyai/unify/tree/2ac6a32dae5e9f5a6e850411d6fe7ab3d386206c/unify/universal_api/clients/uni_llm.py#L464)</p>

```python
def __str__(self):
```

