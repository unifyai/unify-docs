---
title: 'Datasets'
---

One common use case of contexts is the creation of *datasets*. Each dataset has it's own context, such that it can easily be viewed in it's own table.

The `unify.Dataset` class is the best way to interact with your datasets from the Python client.

```python
my_dataset = unify.Dataset([0, 1, 2], name="my_dataset")
```

You can then upload this to your interface like so.

```python
my_dataset.upload()
```

`unify.Dataset` sets the context as `f"Datasets/{name}"`, so your dataset can be viewed at `Datasets/my_dataset`.

IMG

If your dataset already exists upstream, you can download it like so.

```python
my_dataset = unify.Dataset.from_upstream("my_dataset")
```

You can also add and remove content by value using standard Python operators.

```python
my_dataset += [3, 4, 5]
my_dataset -= [0, 1, 2]
my_dataset.upload(overwrite=True)
```

Given that logs can be shared across multiple contexts, it's easy to create sub-datasets withou duplication.

```python
import random
my_dataset = unify.Dataset(
    list(range(100)), name="my_dataset"
).upload()
my_sub_dataset = unify.Dataset(
    random.sample(my_dataset, 10), name="my_sub_dataset"
).upload()
```

IMG

Editing a log (row) in one dataset will be reflected in the other.

IMG

Logs can also be removed from one dataset table, without deleting it from the other.
However, if the log is deleted from both datasets, then it will be deleted entirely.

IMG