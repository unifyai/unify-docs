---
title: 'Improve Agent'
---

In the last section we created test sets of varying sizes,
ready to evaluate our agent.
So, it's finally time to start our data flywheel spinning!
The general process for optimizing an LLM agent is quite straightforward:

```
1Ô∏è‚É£ Create simplest possible agent ü§ñ
2Ô∏è‚É£ While True:
3Ô∏è‚É£     Create/expand unit tests (evals) üóÇÔ∏è
4Ô∏è‚É£     While run(tests) failing: üß™
5Ô∏è‚É£        Analyze failures, understand the root cause üîç
6Ô∏è‚É£        Vary system prompt, in-context examples, tools etc. to rectify üîÄ
7Ô∏è‚É£    [Optional] Beta test with users, find more failures üö¶
```

Firstly, let's activate the `MarkingAssistant` project.

```python
unify.activate("MarkingAssistant")
```

Let's also set a new context `Evals`,
where we'll store all of our evaluation runs.

```python
unify.set_context("Evals")
```

Let's now go through this data flywheel step-by-step!

### ü§ñ Create Agent

Let's start with a simple 0-shot LLM to begin with.

```python
agent = unify.Unify("o3-mini@openai", traced=True)
```

Let's also download a `.cache.json` file which was previously generated whilst running this notebook,
to avoid making any real LLM calls,
and to also make our walkthrough deterministic.

If you'd rather go down **your own unique iteration journey**,
then you should skip the cell below,
and either remove `cache="read-only"` (turn off caching)
or replace it with `cache=True` (create your own local cache) in the agent constructor above.
However,
this would mean many parts of the remaining walkthrough might not directly apply in your case,
as the specific failure modes and the order in which they appear are likely to be different.

```python
if os.path.exists(".cache.json"):
    os.remove(".cache.json")
wget.download(
    "https://raw.githubusercontent.com/"
    "unifyai/demos/refs/heads/main/"
    "marking_assistant/.cache.json"
)
```

The agent needs to mark student answers to questions,
out of a possible maximum number of marks.
Let's give it a sensible system message to begin with:

```
system_message = """
Your task is to award a suitable number of marks for a student's answer to a question, from 0 up to a maximum of {available_marks_total} marks.

The question is:

{question}


Their answer to this question is:

{answer}


As the very final part of your response, simply provide the number of marks on a *new line*, without any additional formatting. For example:

3
"""
```

Let's wrap our system prompt in a simple function so the system message is updated based on the specific data involved:

```python
@unify.traced
def call_agent(system_msg, question, answer, available_marks_total):
    local_agent = agent.copy()
    local_agent.set_system_message(
        system_msg.replace(
            "{question}", question
        ).replace(
            "{answer}", json.dumps(answer, indent=4)
        ).replace(
            "{available_marks_total}", str(available_marks_total)
        )
    )
    return local_agent.generate()
```

## üóÇÔ∏è Add Tests

Great, we now have our agent implemented.
So, what are some good unit tests to begin with?
Rather than using all 321 examples for our first iteration.
Let's use the smallest subset of 10 examples,
which we created in the previous section.

## üß™ Run Tests

Let's add evaluation function,
and include all other arguments that we would like to log as part of the evaluation.
All input arguments, intermediate variables,
and return variables without a prepending `"_"` in the name
(all "non-private" arguments, returns and variables)
will automatically be logged when the function is called,
with the use of a `unify.log` decorator.

```python
@unify.log
def evaluate(
    question,
    student_answer,
    available_marks_total,
    correct_marks_total,
    per_question_breakdown,
    _system_message,
):
    pred_marks = call_agent(
        _system_message, question, student_answer,
        available_marks_total
    )
    _pred_marks_split = pred_marks.split("\n")
    pred_marks_total, diff_total, error_total = None, None, None
    for _substr in reversed(_pred_marks_split):
        _extracted = "".join([c for c in _substr if c.isdigit()])
        if _extracted != "":
          pred_marks_total = int(_extracted)
          diff_total = correct_marks_total - pred_marks_total
          error_total = abs(diff_total)
          break
    pred_marks = {"_": {"marks": pred_marks_total, "rationale": pred_marks}}
    return error_total
```

We can then run our evaluation,
with the logging included,
like so:

```python
with unify.Experiment("simple_agent"), unify.Params(
    system_message=system_message,
    dataset="dataset_10"
):
    unify.map(
        evaluate,
        [
             dict(**d, _system_message=system_message)
             for d in dataset_10
        ],
        name="Evals"
    )
```