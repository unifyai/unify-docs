---
title: 'Remaining Iterations [WIP]'
---

## 🔁 Iteration 6

### 🔍 Still Ignoring Mark Types

Let's try to *force* the agent to reason about each potential mark mentioned in the markscheme, by further refining our structured output. Let's expand upon the `reasoning` field for each sub-question, with a field for each mark type referenced in the sub-question markscheme, going from the following structure:

```
Prediction:
  a:
    reasoning: str
    marks: int
  b:
    reasoning: str
    marks: int
  ...
```

To this version which explicitly enforces reasoning about each potential mark type referenced in the markscheme:

```
Prediction:
  a:
    reasoning:
      B1:
        thoughts: str
        should_award: bool
      SC1:
        thoughts: str
        should_award: bool
      ...,
      overall_thoughts: str
    marks: int
  b:
    reasoning:
      M1:
        thoughts: str
        should_award: bool
      A1:
        thoughts: str
        should_award: bool
      ...,
      overall_thoughts: str
    marks: int
  ...
```


This way, the agent will be **forced** to reason about `SC1` for **Example 207**, `M1` for **Example 261**, and `B1` for **Example 132 (c)**.

### 🔀 Mark Type Reasoning

Let's first define a function to dynamically construct the required pydantic type. For each parsed mark type, we want the model to give it's thoughts and make a decision as to whether or not the mark should be awarded. Let's create this pydantic type first:

```python
class ThoughtsAndAwardDecision(BaseModel):
    thoughts: str
    should_award: bool
```

Let's then create a function to dynamically construct a `PerMarkReasoning` pydantic type, with one `ThoughtsAndAwardDecision` instance for each mark detected in the sub-question markscheme.

```python
@unify.traced(name="create_per_mark_reasoning_format_{mark_types}")
def create_per_mark_reasoning_format(mark_types):
    response_fields = dict(
        zip(
            mark_types + ["overall_thoughts"], [(ThoughtsAndAwardDecision, ...)] * len(mark_types) + [(str, ...)]
        )
    )
    return create_model('PerMarkReasoning', **response_fields)
```

Let's then re-define `MarksAndReasoning` (previously this was statically defined, see above) such that the `reasoning` field is no longer just a string, but is intead our newly created `PerMarkReasoning` (above).

```python
@unify.traced(name="create_marks_and_reasoning_format_{mark_types}")
def create_marks_and_reasoning_format(mark_types):
    return create_model(
        'MarksAndReasoning',
        reasoning=(create_per_mark_reasoning_format(mark_types), ...),
        marks=(int, ...)
    )
```

Finally, let's then update the top-level function `create_response_format` such that we're making use of our newly defined `create_marks_and_reasoning_format` for each sub-question.

```python
@unify.traced(name="create_response_format_{response_keys}_{mark_types}")
def create_response_format(response_keys, mark_types):
    if response_keys:
        response_fields = dict(
            zip(
                response_keys,
                [
                    (create_marks_and_reasoning_format(mark_types[key]), ...)
                    for key in response_keys
                ]
            )
        )
        return create_model('Response', **response_fields)
    else:
        return create_marks_and_reasoning_format(mark_types["_"])
```

We also need to write a function to parse the relevant marks from each sub-question markscheme. We can take inspiration from [here](https://colab.research.google.com/drive/1mWlZxuL8jyl-C--F_vmtIHq1cIm4poxG#scrollTo=aLB8DdP__KlC&line=6&uniqifier=1), where we wrote a parser for a slightly different reason. Let's have the function extract the marks, and also the surrounding context.

```python
@unify.traced
def parse_marks_from_markscheme(markscheme: str):
    extracted_marks = re.findall(r'(?:SC|M|A|B)\d+', markscheme)
    if not extracted_marks:
        return []
    marks_n_context = list()
    for i, mark in enumerate(extracted_marks):
        index = markscheme.find(mark)
        chunk = markscheme[0:index]
        if i > 0:
            prev_mark = extracted_marks[i-1]
            marks_n_context[i-1][1] += chunk
        markscheme = markscheme[index:]
        marks_n_context.append([mark, chunk])
    marks_n_context[-1][1] += markscheme
    return marks_n_context
```

Finally, we'll also need to update `call_agent` such that we call `parse_available_marks_from_markscheme` on each sub-question markscheme, and then pass these into our newly defined `create_response_format`.

```python
@unify.traced
def call_agent(system_msg, question, sub_questions, markscheme, answer, available_marks_total):
    local_agent = agent.copy()
    with_subqs = len(markscheme) > 1
    response_format = create_response_format(
        list(markscheme.keys()) if with_subqs else None,
        {k: [itm[0] for itm in parse_marks_from_markscheme(v)]
        for k, v in markscheme.items()}
    )
    local_agent.set_response_format(response_format)
    if with_subqs:
        output_response_exp = output_response_explanations["with_subqs"]
        output_response_exp = output_response_exp.replace(
            "{subquestions}", json.dumps(list(markscheme.keys()))
        )
    else:
        output_response_exp = output_response_explanations["without_subqs"]
    markscheme = {k: update_markscheme(v) for k, v in markscheme.items()}
    local_agent.set_system_message(
        system_msg.replace(
            "{question}", question
        ).replace(
            "{markscheme}", json.dumps(markscheme, indent=4)
        ).replace(
            "{answer}", json.dumps(answer, indent=4)
        ).replace(
            "{available_marks_total}", str(available_marks_total)
        ).replace(
            "{questions_markscheme_and_answers}", json.dumps(
              {
                  k: {
                      "sub-question": sub_questions[k],
                      "markscheme": markscheme[k],
                      "answer": answer[k]
                  } for k in sub_questions.keys()
              },
              indent=4
            )
        ).replace(
            "{output_response_explanation}",
            output_response_exp
        )
    )
    ret = local_agent.generate()
    if "```" in ret:
        ret = ret.split("```")[-2].lstrip("json")
    ret = response_format.model_validate_json(ret).model_dump()
    if not with_subqs:
        return {"_": ret}
    return ret
```

Let's also update our system message to explain to better explain to the agent how it should reason about this new output structure.

```python
output_response_explanations = dict()
output_response_explanations["with_subqs"] = "For each sub-question {subquestions}, you should populate the `reasoning` field with your general thoughts on each individual mark identified in the markscheme, and also a decision as to whether each of these mark should be awarded. These marks are not necessarily cumulative with regards to the marks to award, and some may be irrelevant given the student's approach or answer, in which case just respond `False` for the `should_award` field. Finally, you should put the total number of marks to award for each sub-question in the corresponding `marks` field."
output_response_explanations["without_subqs"] = "You should populate the `reasoning` field with your general thoughts on each individual mark identified in the markscheme, and also a decision as to whether each of these mark should be awarded. These marks are not necessarily cumulative with regards to the marks to award, and some may be irrelevant given the student's approach or answer, in which case just respond `False` for the `should_award` field. Finally, you should put the total number of marks to award in the `marks` field."
```

We also need to update `evaluate`, given some of the other changes.

```python
@unify.log
def evaluate(
    question,
    sub_questions,
    student_answer,
    available_marks_total,
    markscheme,
    correct_marks,
    per_question_breakdown,
    _system_message,
):
    pred_marks = call_agent(
        _system_message, question, sub_questions, markscheme, student_answer,
        available_marks_total
    )
    pred_marks_total = sum([v["marks"] for v in pred_marks.values()])
    diff = {
        k: vcor["marks"] - vpred["marks"] for (k, vcor), (_, vpred) in
        zip(correct_marks.items(), pred_marks.items())
    }
    error = {k: abs(v) for k, v in diff.items()}
    diff_total = sum(diff.values())
    error_total = sum(error.values())
    per_question_breakdown = {
        k: {
            **per_question_breakdown[k],
            "predicted_marks": pm,
            "diff": d
        } for (k, pqb), pm, d in zip(
            per_question_breakdown.items(),
            pred_marks.values(),
            diff.values()
        )
    }
    return error
```

### 🧪 Rreun Tests

```python {1}
with unify.Experiment("mark_type_reasoning", overwrite=True), unify.Params(
    system_message=system_message,
    dataset="TestSet10",
    source=unify.get_source()
):
    unify.map(
        evaluate,
        [
             dict(**d.entries, _system_message=system_message)
             for d in test_set_10
        ],
        name="Evals"
    )
```

The failure modes are **still entirely unchanged!** `o3-mini` is certainly very stubborn about it's decision for these questions.

## 🔁 Iteration 7

### 🔍 Still Ignoring Mark Types

Let's see what affect our new output format had on the nature of the agent's responses, if any.

Considering **Example 207**, the agent still failed to award `SC1` for the student's answer `1/3, 0.34, 3.5%`, despite the markscheme explicitly stating `SC1 for 1/3, 0.34, 3.5%`. The agent's explicit `thoughts` about `SC1` were:

> 🤖 No special case credit is applicable here since the order is incorrect and no alternative acceptable method is demonstrated.

This is a pretty fluffy and empty statement. Despite o3-mini being a multi-step reasoning model, perhaps we're *still* asking the agent to consider too many things at once.

Enforcing the agent to consider one mark at a time might rectify this lack of attention to detail.

**Example 132** is even more difficult, where the agent not only needs to consider each mark, but it also has six different sub-questions to reason about, each with their own set of available marks and mark types.

Let's see if using a separate LLM call per sub-question improves the performance on **Example 132**.

### 🔀 Queries per Subquestion

Firstly, let's create a new system prompt for our agent, which will reason about one-subquestion at a time.

```python
system_message = """
Your task is to award a suitable number of marks for a student's answer to a question, from 0 up to a maximum of {available_marks} marks.

The general marking guidelines (relevant for all questions) are as follows:

{general_guidelines}


The question you need to mark is:

{question}


Their answer is:

{answer}


The markscheme is:

{markscheme}


{output_response_explanation}
""".replace(
    "{general_guidelines}",
    general_guidelines
)
```

Given the changes, we can also remove the `output_response_explanations` dict, and relace it with a single `output_response_explanation` string variable, given that the agent no longer needs to output responses for multiple sub-questions in a single response.

```python
output_response_explanation = "You should populate the `reasoning` field with your general thoughts on each individual mark identified in the markscheme, and also a decision as to whether each of these mark should be awarded. These marks are not necessarily cumulative with regards to the marks to award, and some may be irrelevant given the student's approach or answer, in which case just respond `False` for the `should_award` field. Finally, you should put the total number of marks to award in the `marks` field."
```

Let's update `call_agent` to map each subquestion to a unique LLM call, and let's also add a `unify.traced` decorator so we're able to capture each individual LLM call in the overall trace.

```python
@unify.traced
def call_agent(example_id, system_msg, sub_questions, markscheme, answer, available_marks):
    agents = {k: agent.copy() for k in markscheme.keys()}
    with_subqs = len(markscheme) > 1
    response_formats = {
        k: create_marks_and_reasoning_format(
            [itm[0] for itm in parse_marks_from_markscheme(v)]
        ) for k, v in markscheme.items()
    }
    [
        agnt.set_response_format(rf)
        for agnt, rf in zip(
            agents.values(), response_formats.values()
        )
    ]
    markscheme = {
        k: update_markscheme(v) for k, v in markscheme.items()
    }
    for k in markscheme.keys():
        agents[k].set_system_message(
            system_msg.replace(
                "{question}", sub_questions[k]
            ).replace(
                "{markscheme}", markscheme[k]
            ).replace(
                "{answer}", answer[k]
            ).replace(
                "{available_marks}",
                str(available_marks[k.replace("_", "total")])
            ).replace(
                "{output_response_explanation}",
                output_response_explanation
            )
        )
    rets = unify.map(
        lambda a: a.generate(),
        list(agents.values()),
        name=f"Evals[{example_id}]->SubQAgent"
    )
    rets = [
        ret.split("```")[-2].lstrip("json")
        if "```" in ret else ret
        for ret in rets
    ]
    rets = {
        k: response_formats[k].model_validate_json(ret).model_dump()
        for k, ret in zip(markscheme.keys(), rets)
    }
    return rets
```

Let's also update `evaluate` to pass the updated parameters to `call_agent`.

```python
@unify.log
def evaluate(
    example_id,
    question,
    sub_questions,
    student_answer,
    available_marks,
    markscheme,
    correct_marks,
    per_question_breakdown,
    _system_message,
):
    pred_marks = call_agent(
        example_id, _system_message, sub_questions, markscheme, student_answer,
        available_marks
    )
    pred_marks_total = sum([v["marks"] for v in pred_marks.values()])
    diff = {
        k: vcor["marks"] - vpred["marks"] for (k, vcor), (_, vpred) in
        zip(correct_marks.items(), pred_marks.items())
    }
    error = {k: abs(v) for k, v in diff.items()}
    diff_total = sum(diff.values())
    error_total = sum(error.values())
    per_question_breakdown = {
        k: {
            **per_question_breakdown[k],
            "predicted_marks": pm,
            "diff": d
        } for (k, pqb), pm, d in zip(
            per_question_breakdown.items(),
            pred_marks.values(),
            diff.values()
        )
    }
    return error
```

### 🧪 Rreun Tests

```python {1}
with unify.Experiment("queries_per_subquestion", overwrite=True), unify.Params(
    system_message=system_message,
    dataset="TestSet10",
    source=unify.get_source()
):
    unify.map(
        evaluate,
        [
             dict(**d.entries, _system_message=system_message)
             for d in test_set_10
        ],
        name="Evals"
    )
```

## 🔁 Iteration 8

### 🔍 Lack of Global Context

These updates have actually regressed the overall performance, with us now having a mean error of `0.5`.

Maybe the purely local reasoning has some shortcomings. Let's focus on one of the **new regressions**, to understand why our latest change has distrupted the agent where it was previously very consistently correct.

**Example 20 (b)**

❓ Question [1 Mark]:

> Explain why he might not want to use Account A.

📝 Student's Answer:

> He might not want to use Account A because the bank might keep the money forever and he could lose it.

☑️ Markscheme:

> He may not want to leave it there for 3 years. Accept any valid reason.

✅ Correct Marks [0/1] Rationale:

> The markscheme awards 1 mark for a valid reason such as not wanting to lock away funds for 3 years. Here, the stated reason is irrelevant and does not accurately reflect the restrictions of Account A.

🤖 Predicted Marks [1/1] ❌ Rationale:

> The mark scheme awards 1 mark for any valid reason why the person might not want to leave the money in Account A for 3 years. The student's answer, although phrasing it as the bank might keep the money forever and thus he could lose it, is a valid reason to be cautious, even though it doesn't mention the explicit time period of 3 years. This interpretation sufficiently aligns with the mark scheme's requirement of a valid reason. Therefore, the mark is awarded.

Clearly, the agent is now taking some liberties with what constitutes a "valid reason". It's unclear why the agent is only making these mistakes now.

Let's look at some of the previous justifications for **not** awarding the mark, from the previous runs.

🤖 Predicted Marks [x/1] Rationales:

> `simple_agent` [0/1] ✅: The answer should explain that although Account A gives slightly more money, Derrick might not want this account because his money is tied up for three years (i.e. it has poor liquidity). The student's explanation about the bank "keeping the money forever" is not an acceptable financial rationale.

> `add_markscheme` [0/1] ✅: The explanation provided is not a valid reason and does not address the restriction of no withdrawals for three years. Therefore, no mark can be awarded for this part either.

> `add_marking_guidelines` [0/1] ✅: The explanation given ("because the bank might keep the money forever and he could lose it") does not correctly address why he might not want to use Account A (namely, that his money is locked away for 3 years with no withdrawals allowed).

> `add_structured_output` [0/1] ✅: The answer needed to explain why Account A might not be desirable, which is because funds are locked in for three years and withdrawals are not allowed until the end of the period. The student's answer states that the bank might keep the money forever and he could lose it, which does not address the actual restriction (lack of liquidity). This is an irrelevant or incorrect reason and does not meet the mark scheme.

> `align_context` [0/1] ✅: The mark scheme accepts any valid reason such as not being able to withdraw the money because it is locked in for 3 years. The candidate's answer – that the bank might 'keep the money forever' and he could lose it – is not a valid explanation for the funds being inaccessible. It does not clearly express that his money is tied up for 3 years. Therefore, this explanation does not meet the requirement and no mark should be awarded.

> `align_guidelines_and_clarify_reasoning` [0/1] ✅: The answer to part (b) should explain that Account A requires the money to be locked in for three years which might be a disadvantage if immediate access is desired. The student's explanation – that the bank might 'keep the money forever' – does not address the real issue of the restriction on withdrawals. As the explanation provided is not a valid reason as per the mark scheme, no marks should be awarded for this part.

> `mark_type_reasoning` [0/1] ✅: The explanation provided does not address the key issue mentioned in the mark scheme (the restriction of no withdrawals for 3 years) and instead gives a non sequitur reason that the bank might 'keep the money forever.' This does not meet the required rationale, so no mark is awarded.

Perhaps preventing the agent from having access to the full question prevents it from using "common sense" and realizing how "silly" the proposed answer is, in light of the overall question and the information provided to the student.

Maybe strict adherence to the markscheme alone without the full context is prohibitive.

Let's update our per-subquestion system prompts to also fully include the **preceeding** sub-questions, their markschemes, and their answers. It's unlikely that the context of a **later** question will assists with the marking of an earlier question, and we still want to try and keep the agent as focused as possible on the relevant information.

### 🔀 Include Preceeding Context

Let's first update the system prompt, re-introducing the placeholder for the aligned subquestions, markschemes and answers, this time calling it `{prior_context}`, which will only be included when sub-questions are present. Let's also include the full question.

```python
system_message = """
Your task is to award a suitable number of marks for a student's answer to question {subq}, from 0 up to a maximum of {available_marks} marks.

The general marking guidelines (relevant for all questions) are as follows:

{general_guidelines}


The *overall* question is:

{question}

{prior_context}

The specific question you need to mark is:

{subquestion}


Their answer to this specific question is:

{answer}


The markscheme for this specific question is:

{markscheme}


{output_response_explanation}
""".replace(
    "{general_guidelines}",
    general_guidelines
)
```

Let's also add a general explanation for the prior context, in cases where it is included.

```python
prior_context_exp = """
All of the *preceeding* sub-questions, their specific markschemes and the student's answers are as follows:
"""
```

Let's now update `call_agent` to pass in the required information.

```python
@unify.traced
def call_agent(example_id, system_msg, question_num, question, sub_questions, markscheme, answer, available_marks):
    agents = {k: agent.copy() for k in markscheme.keys()}
    with_subqs = len(markscheme) > 1
    response_formats = {
        k: create_marks_and_reasoning_format(
            [itm[0] for itm in parse_marks_from_markscheme(v)]
        ) for k, v in markscheme.items()
    }
    [
        agnt.set_response_format(rf)
        for agnt, rf in zip(
            agents.values(), response_formats.values()
        )
    ]
    markscheme = {
        k: update_markscheme(v) for k, v in markscheme.items()
    }
    for i, k in enumerate(markscheme.keys()):
        agents[k].set_system_message(
            system_msg.replace(
                "{subq}", k.replace("_", str(question_num))
            ).replace(
                "{question}", question,
            ).replace(
                "{subquestion}", sub_questions[k]
            ).replace(
                "{markscheme}", markscheme[k]
            ).replace(
                "{answer}", answer[k]
            ).replace(
                "{available_marks}",
                str(available_marks[k.replace("_", "total")])
            ).replace(
                "{output_response_explanation}",
                output_response_explanation
            ).replace(
            "{prior_context}", (prior_context_exp + json.dumps(
              {
                  k: {
                      "sub-question": sub_questions[k],
                      "markscheme": markscheme[k],
                      "answer": answer[k]
                  } for k in list(sub_questions.keys())[0:i]
              },
              indent=4
            )) if with_subqs and i > 0 else ""
          )
        )
    rets = unify.map(
        lambda a: a.generate(),
        list(agents.values()),
        name=f"Evals[{example_id}]->SubQAgent"
    )
    rets = [
        ret.split("```")[-2].lstrip("json")
        if "```" in ret else ret
        for ret in rets
    ]
    rets = {
        k: response_formats[k].model_validate_json(ret).model_dump()
        for k, ret in zip(markscheme.keys(), rets)
    }
    return rets
```

Finally, let's update `evaluate` accordingly.

```python
@unify.log
def evaluate(
    example_id,
    question_num,
    question,
    sub_questions,
    student_answer,
    available_marks,
    available_marks_total,
    markscheme,
    correct_marks,
    correct_marks_total,
    per_question_breakdown,
    _system_message,
):
    pred_marks = call_agent(
        example_id, _system_message, question_num, question, sub_questions, markscheme, student_answer, available_marks
    )
    pred_marks_total = sum([v["marks"] for v in pred_marks.values()])
    diff = {
        k: vcor["marks"] - vpred["marks"] for (k, vcor), (_, vpred) in
        zip(correct_marks.items(), pred_marks.items())
    }
    error = {k: abs(v) for k, v in diff.items()}
    diff_total = sum(diff.values())
    error_total = sum(error.values())
    per_question_breakdown = {
        k: {
            **per_question_breakdown[k],
            "predicted_marks": pm,
            "diff": d
        } for (k, pqb), pm, d in zip(
            per_question_breakdown.items(),
            pred_marks.values(),
            diff.values()
        )
    }
    return error
```

### 🧪 Rreun Tests

```python {1}
with unify.Experiment("with_preceeding_context", overwrite=True), unify.Params(
    system_message=system_message,
    dataset="TestSet10",
    source=unify.get_source()
):
    unify.map(
        evaluate,
        [
             dict(**d.entries, _system_message=system_message)
             for d in test_set_10
        ],
        name="Evals"
    )
```

Great, so we've fixed the new regressions, but again we're back at the same three failures, failing for the same reason.

## 🔁 Iteration 9

### 🔍 Still Ignoring Mark Types

Given that the agent is **still** failing to follow the instructions for each mark in the markscheme, perhaps it's time we tried to perform per-mark reasoning, with a separate LLM call made for each candidate mark to award. This might help the LLM deeply consider each candidate mark mentioned in the markscheme.

Let's give it a try!

### 🔀 Queries per Mark

We will still want our per-subquestion LLM to perform the final reasoning about the number of marks to award for the sub-question, but we just want to provide it with the reasoning performed by each of our per-mark LLM queries.

We therefore now have two different LLMs, with two different roles, and therefore we need two different system messages.

Let's first update the subquestion system message, in anticipation of the incoming mark-by-mark reasoning. Let's also split the markscheme and the mark type reasoning, rather than naively combining these as was done in `update_markscheme`.

```python
subq_system_message = """
Your task is to award a suitable number of marks for a student's answer to question {subq}, from 0 up to a maximum of {available_marks} marks.

The general marking guidelines (relevant for all questions) are as follows:

{general_guidelines}


The *overall* question is:

{question}

{prior_context}

The specific question you need to mark is:

{subquestion}


Their answer to this specific question is:

{answer}


The markscheme for this specific question is:

{markscheme}

{mark_types_explanation}

{mark_observations}

{output_response_explanation}
""".replace(
    "{general_guidelines}",
    general_guidelines
)
```

The `"{mark_types_explanation}"` placeholder can be overriden explicitly, giving us more control. Let's create a new function `extract_mark_type_explanation`, inspired from `update_markscheme` above.

```python
@unify.traced(name="extract_mark_type_explanation_{marks_to_consider}")
def extract_mark_type_explanation(markscheme: str, marks_to_consider=None):
    m_marks = sorted(list(set(re.findall(r'M\d+', markscheme))))
    a_marks = sorted(list(set(re.findall(r'A\d+', markscheme))))
    b_marks = sorted(list(set(re.findall(r'B\d+', markscheme))))
    sc_marks = sorted(list(set(re.findall(r'SC\d+', markscheme))))
    if not any(m_marks + a_marks + b_marks + sc_marks):
        return ""
    full_exp = """As a recap, the general guidelines for each of these mark types are as follows:

{mark_types_explanation}"""
    for marks in (m_marks, a_marks, b_marks, sc_marks):
        for mark in marks:
            if marks_to_consider and mark not in marks_to_consider:
                continue
            key = "".join(c for c in mark if not c.isdigit())
            num_marks = int("".join(c for c in mark if c.isdigit()))
            exp = mark_types[key]
            exp = exp.replace(
                "{num}", str(num_marks)
            ).replace(
                "{num_marks}", "1 mark" if num_marks == 1 else f"{num_marks} marks"
            )
            full_exp = full_exp.replace(
                "{mark_types_explanation}",
                key + ":/n" + exp + "\n\n{mark_types_explanation}"
            )
    return full_exp.replace("{mark_types_explanation}", "")
```

Let's now create the system message for our mark reasoning agent, again with the explicit `{mark_types_explanation}` placeholder.

```python
mark_system_message = """
Your task is to determine whether mark {mark} should be awarded for the following student's answer to question {subq}, based on the provided markscheme.

The general marking guidelines (relevant for all questions) are as follows:

{general_guidelines}


The *overall* question is:

{question}

{prior_context}

The specific question you need to mark is:

{subquestion}


Their answer to this specific question is:

{answer}


The markscheme for this specific question, with the mark in question {mark} expressed in bold and with a prepending `(to consider!)`, is as follows:

{markscheme}

{mark_types_explanation}

You should populate the `thoughts` field with your thoughts on the whether the specific mark identified within the markscheme should be awarded for the student's answer. The mark might be irrelevant given the student's approach or answer, in which case just respond `False` for the `should_award` field, and explain this in the `thoughts` field. Please think carefully about your decision for the mark, considering the general guidelines.
""".replace(
    "{general_guidelines}",
    general_guidelines
)
```

Let's first define `call_subq_agent`, which will include mark-by-mark reasoning with several LLM calls

```python
@unify.traced(name="call_subq_agent_{subq}")
def call_subq_agent(example_id, subq, subq_agent, markscheme, mark_sys_msg):
    parsed_markscheme = parse_marks_from_markscheme(markscheme)
    mark_agents = [
        [k, agent.copy()] for k in
        [itm[0] for itm in parsed_markscheme]
    ]
    [
        agnt.set_response_format(ThoughtsAndAwardDecision)
        for _, agnt in mark_agents
    ]
    for i, (k, v) in enumerate(parsed_markscheme):
        mark_agents[i][1].set_system_message(
            mark_sys_msg.replace(
                "{mark}", k
            ).replace(
                "{markscheme}", markscheme
            ).replace(
                v, v.replace(k, f"**{k}** (to consider!)")
            ).replace(
                "{mark_types_explanation}",
                extract_mark_type_explanation(markscheme, [k])
            )
        )
    if mark_agents:
        explanation = "An expert marker has already taken a look at the student's answer, and they have made the following observations for each of the candidate marks mentioned in the markscheme. You should pay special attention to these observations."
        vals = unify.map(
            lambda a: json.loads(a.generate()),
            [agnt for _, agnt in mark_agents],
            name=f"Evals[{example_id}]->SubQAgent[{subq}]->MarkAgent"
        )
        keys = list()
        for k, _ in mark_agents:
            if k not in keys:
                keys.append(k)
                continue
            keys.append(
                k + f"({len([ky for ky in keys if k in ky])})"
            )
        mark_obs_dict = dict(zip(keys, vals))
        mark_observations = explanation + "\n\n" + json.dumps(
            mark_obs_dict, indent=4
        )
    else:
        mark_observations = ""
    subq_agent.set_system_message(
        subq_agent.system_message.replace(
            "{mark_observations}",
            mark_observations
        )
    )
    ret = subq_agent.generate()
    if "```" in ret:
        ret = ret.split("```")[-2].lstrip("json")
    ret = json.loads(ret)
    if not mark_agents:
        return ret
    ret["reasoning"] = {
        **mark_obs_dict,
        "overall_thoughts": ret["reasoning"]
    }
    return ret
```

Let's now update `call_agent`, making use of our `call_subq_agent` function, which processes a single sub-question.

```python
@unify.traced
def call_agent(
    example_id,
    subq_system_message,
    mark_system_message,
    question_num,
    question,
    sub_questions,
    markscheme,
    answer,
    available_marks
):
    subq_agents = {k: agent.copy() for k in markscheme.keys()}
    with_subqs = len(markscheme) > 1
    response_formats = {
        k: MarksAndReasoning for k, v in markscheme.items()
    }
    [
        agnt.set_response_format(rf)
        for agnt, rf in zip(
            subq_agents.values(), response_formats.values()
        )
    ]
    mark_sys_msgs = list()
    for i, k in enumerate(markscheme.keys()):
        subq_agents[k].set_system_message(
            subq_system_message.replace(
                "{subq}", k.replace("_", str(question_num))
            ).replace(
                "{question}", question,
            ).replace(
                "{subquestion}", sub_questions[k]
            ).replace(
                "{markscheme}", markscheme[k]
            ).replace(
                "{mark_types_explanation}",
                extract_mark_type_explanation(markscheme[k])
            ).replace(
                "{answer}", answer[k]
            ).replace(
                "{available_marks}",
                str(available_marks[k.replace("_", "total")])
            ).replace(
                "{output_response_explanation}",
                output_response_explanation
            ).replace(
            "{prior_context}", (prior_context_exp + json.dumps(
              {
                  k: {
                      "sub-question": sub_questions[k],
                      "markscheme": markscheme[k],
                      "answer": answer[k]
                  } for k in list(sub_questions.keys())[0:i]
              },
              indent=4
            )) if with_subqs and i > 0 else ""
          )
        )
        mark_sys_msgs.append(
            mark_system_message.replace(
                "{subq}", k.replace("_", str(question_num))
            ).replace(
                "{question}", question,
            ).replace(
                "{subquestion}", sub_questions[k]
            ).replace(
                "{answer}", answer[k]
            ).replace(
            "{prior_context}", (prior_context_exp + json.dumps(
              {
                  k: {
                      "sub-question": sub_questions[k],
                      "markscheme": markscheme[k],
                      "answer": answer[k]
                  } for k in list(sub_questions.keys())[0:i]
              },
              indent=4
            )) if with_subqs and i > 0 else ""
          )
        )
    rets = unify.map(
        lambda *a: call_subq_agent(example_id, *a),
        list(sub_questions.keys()),
        list(subq_agents.values()),
        list(markscheme.values()),
        mark_sys_msgs,
        from_args=True,
        name=f"Evals[{example_id}]->SubQAgent"
    )
    return dict(zip(markscheme.keys(), rets))
```

We also need to update the `evaluate` function, to pass each of the two different system messages to the `call_agent` function.

```python
@unify.log
def evaluate(
    example_id,
    question_num,
    question,
    sub_questions,
    student_answer,
    available_marks,
    available_marks_total,
    markscheme,
    correct_marks,
    correct_marks_total,
    per_question_breakdown,
    _subq_system_message,
    _mark_system_message
):
    pred_marks = call_agent(
        example_id,
        _subq_system_message,
        _mark_system_message,
        question_num,
        question,
        sub_questions,
        markscheme,
        student_answer,
        available_marks
    )
    pred_marks_total = sum([v["marks"] for v in pred_marks.values()])
    diff = {
        k: vcor["marks"] - vpred["marks"] for (k, vcor), (_, vpred) in
        zip(correct_marks.items(), pred_marks.items())
    }
    error = {k: abs(v) for k, v in diff.items()}
    diff_total = sum(diff.values())
    error_total = sum(error.values())
    per_question_breakdown = {
        k: {
            **per_question_breakdown[k],
            "predicted_marks": pm,
            "diff": d
        } for (k, pqb), pm, d in zip(
            per_question_breakdown.items(),
            pred_marks.values(),
            diff.values()
        )
    }
    return error
```

### 🧪 Rreun Tests

```python
agent.set_cache("read-only") {1-3, 12-13}
with unify.Experiment("queries_per_mark", overwrite=True), unify.Params(
    subq_system_message=subq_system_message,
    mark_system_message=mark_system_message,
    dataset="TestSet10",
    source=unify.get_source()
):
    unify.map(
        evaluate,
        [
             dict(
                 **d.entries,
                 _subq_system_message=subq_system_message,
                 _mark_system_message=mark_system_message
            )
             for d in test_set_10
        ],
        name="Eval",
    )
```

Great, this seems to have addressed two of the three failures (on this run at least).

## 🔁 Iteration 10

### 🔍 Method Marks Confusion

All of the prior failures now seem to have been resolved, but we have a new regression for **Example 215 (b)**. Let's take a look.

**Example 215 (b)**

❓ Question [2 Marks]:

> Complete these rows of boxes using the rule shown above.

📝 Student's Answer:

> The missing boxes are 8, 13, and 21, making the sequence 8, 13, 21, 34, 55.

☑️ Markscheme:

> Answer: 8, 13, 21
Marks: 2
Guidance:
M1 for one correct subtraction of two boxes

✅ Correct Marks [2/2] Rationale:

> Here, we deduce the missing numbers by working backwards from 34 and 55 or by applying the same rule forward. According to the markscheme (2 marks), we get full credit for the fully correct set of missing numbers, with one method mark (M1) for a correct subtraction and a final accuracy mark (A1) for the correct sequence.

🤖 Predicted Marks [0/2] Rationale:

> The candidate's final numerical answer is correct (the missing boxes are 8, 13, and 21), matching the answer in the mark scheme. However, the mark scheme specifically awards M1 for showing a correct subtraction (i.e. working that reverses the addition rule), which is a required part of the method. Since the candidate only provided the final answer without demonstrating any subtraction (or any equivalent working) to show the method, no evidence is provided to award the M1 mark. As A marks (for an accurate answer) depend on obtaining the method mark, no further marks can be awarded. Therefore, despite the correct final answer, the lack of required working means no marks should be awarded.

This is an interesting failure mode. Interestly, the justification for the "correct" (ground truth) marks is wrong. There is no **A1** mark for this question (which *would* depend on a method mark). This is irrelevant in terms of the agent failure (the agent doesn't know the correct marks or rationale), but it's still an interesting observsation regarding our "ground truth" data.

Interestingly, the agent has made the *same mistake* as occurs in the "ground truth" rationale. Our agent presumes the existence of an **A** mark where none were stated. It seems like the agent doesn't understand that correct answers should always earn full marks, unless otherwise explicitly stated. **M1** marks are not necessary to achieve full marks in such cases, unless an **A** mark is specifically referenced.

### 🔀 Clarify Mark + Answer Marks

Let's try to fully clarify these points for the sub-question agent, and re-run the evals.

```python
output_response_explanation = """
You should populate the `reasoning` field with your general thoughts on each individual mark identified in the markscheme, and also a decision as to whether each of these mark should be awarded.

If you deem that a mark *should* be awarded (such SC1, B1, A1 etc.), then it is worth as many marks as appear in the mark type itself (SC1, B1, and A1 are therefore worth 1 mark each, A2 is worth 2 marks etc.). However, these marks are not *necessarily* cumulative with regards to the total marks to award for this sub-question, and some may be irrelevant given the student's approach or answer.

More importantly, full marks should *always* be given for a fully correct answer, unless otherwise *explicitly* stated. For example, a correct answer without any method shown should still get *full marks*, despite the M1 criteria not being met. The only exception to this is explicitly referenced A marks, which do depend on the preceding M marks being awarded.

Finally, after you've given it a lot of thought, you should put the total number of marks to award for this sub-question in the `marks` field.
"""
```

### 🧪 Rreun Tests

```python {1}
with unify.Experiment("clarify_method_marks", overwrite=True), unify.Params(
    subq_system_message=subq_system_message,
    mark_system_message=mark_system_message,
    dataset="TestSet10",
    source=unify.get_source()
):
    unify.map(
        evaluate,
        [
             dict(
                 **d.entries,
                 _subq_system_message=subq_system_message,
                 _mark_system_message=mark_system_message
            )
             for d in test_set_10
        ],
        name="Eval"
    )
```

Great, we've finally got all 10/10 tests passing perfectly 🎉

### For Downloading cache

```python
from google.colab import files
files.download('.cache.json')
```
