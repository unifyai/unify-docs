---
title: 'Iteration 7'
---

### 🔍 Still Ignoring Mark Types

Let's try to *force* the agent to reason about each potential mark mentioned in the markscheme, by further refining our structured output. Let's expand upon the `reasoning` field for each sub-question, with a field for each mark type referenced in the sub-question markscheme, going from the following structure:

```
Prediction:
  a:
    reasoning: str
    marks: int
  b:
    reasoning: str
    marks: int
  ...
```

To this version which explicitly enforces reasoning about each potential mark type referenced in the markscheme:

```
Prediction:
  a:
    reasoning:
      B1:
        thoughts: str
        should_award: bool
      SC1:
        thoughts: str
        should_award: bool
      ...,
      overall_thoughts: str
    marks: int
  b:
    reasoning:
      M1:
        thoughts: str
        should_award: bool
      A1:
        thoughts: str
        should_award: bool
      ...,
      overall_thoughts: str
    marks: int
  ...
```


This way, the agent will be **forced** to reason about `SC1` for **Example 207**, `M1` for **Example 261**, and `B1` for **Example 132 (c)**.

### 🔀 Mark Type Reasoning

Let's first define a function to dynamically construct the required pydantic type. For each parsed mark type, we want the model to give it's thoughts and make a decision as to whether or not the mark should be awarded. Let's create this pydantic type first:

```python
class ThoughtsAndAwardDecision(BaseModel):
    thoughts: str
    should_award: bool
```

Let's then create a function to dynamically construct a `PerMarkReasoning` pydantic type, with one `ThoughtsAndAwardDecision` instance for each mark detected in the sub-question markscheme.

```python
@unify.traced(name="create_per_mark_reasoning_format_{mark_types}")
def create_per_mark_reasoning_format(mark_types):
    response_fields = dict(
        zip(
            mark_types + ["overall_thoughts"], [(ThoughtsAndAwardDecision, ...)] * len(mark_types) + [(str, ...)]
        )
    )
    return create_model('PerMarkReasoning', **response_fields)
```

Let's then re-define `MarksAndReasoning` (previously this was statically defined, see above) such that the `reasoning` field is no longer just a string, but is intead our newly created `PerMarkReasoning` (above).

```python
@unify.traced(name="create_marks_and_reasoning_format_{mark_types}")
def create_marks_and_reasoning_format(mark_types):
    return create_model(
        'MarksAndReasoning',
        reasoning=(create_per_mark_reasoning_format(mark_types), ...),
        marks=(int, ...)
    )
```

Finally, let's then update the top-level function `create_response_format` such that we're making use of our newly defined `create_marks_and_reasoning_format` for each sub-question.

```python
@unify.traced(name="create_response_format_{response_keys}_{mark_types}")
def create_response_format(response_keys, mark_types):
    if response_keys:
        response_fields = dict(
            zip(
                response_keys,
                [
                    (create_marks_and_reasoning_format(mark_types[key]), ...)
                    for key in response_keys
                ]
            )
        )
        return create_model('Response', **response_fields)
    else:
        return create_marks_and_reasoning_format(mark_types["_"])
```

We also need to write a function to parse the relevant marks from each sub-question markscheme. We can take inspiration from [here](https://colab.research.google.com/drive/1mWlZxuL8jyl-C--F_vmtIHq1cIm4poxG#scrollTo=aLB8DdP__KlC&line=6&uniqifier=1), where we wrote a parser for a slightly different reason. Let's have the function extract the marks, and also the surrounding context.

```python
@unify.traced
def parse_marks_from_markscheme(markscheme: str):
    extracted_marks = re.findall(r'(?:SC|M|A|B)\d+', markscheme)
    if not extracted_marks:
        return []
    marks_n_context = list()
    for i, mark in enumerate(extracted_marks):
        index = markscheme.find(mark)
        chunk = markscheme[0:index]
        if i > 0:
            prev_mark = extracted_marks[i-1]
            marks_n_context[i-1][1] += chunk
        markscheme = markscheme[index:]
        marks_n_context.append([mark, chunk])
    marks_n_context[-1][1] += markscheme
    return marks_n_context
```

Finally, we'll also need to update `call_agent` such that we call `parse_available_marks_from_markscheme` on each sub-question markscheme, and then pass these into our newly defined `create_response_format`.

```python
@unify.traced
def call_agent(system_msg, question, sub_questions, markscheme, answer, available_marks_total):
    local_agent = agent.copy()
    with_subqs = len(markscheme) > 1
    response_format = create_response_format(
        list(markscheme.keys()) if with_subqs else None,
        {k: [itm[0] for itm in parse_marks_from_markscheme(v)]
        for k, v in markscheme.items()}
    )
    local_agent.set_response_format(response_format)
    if with_subqs:
        output_response_exp = output_response_explanations["with_subqs"]
        output_response_exp = output_response_exp.replace(
            "{subquestions}", json.dumps(list(markscheme.keys()))
        )
    else:
        output_response_exp = output_response_explanations["without_subqs"]
    markscheme = {k: update_markscheme(v) for k, v in markscheme.items()}
    local_agent.set_system_message(
        system_msg.replace(
            "{question}", question
        ).replace(
            "{markscheme}", json.dumps(markscheme, indent=4)
        ).replace(
            "{answer}", json.dumps(answer, indent=4)
        ).replace(
            "{available_marks_total}", str(available_marks_total)
        ).replace(
            "{questions_markscheme_and_answers}", json.dumps(
              {
                  k: {
                      "sub-question": sub_questions[k],
                      "markscheme": markscheme[k],
                      "answer": answer[k]
                  } for k in sub_questions.keys()
              },
              indent=4
            )
        ).replace(
            "{output_response_explanation}",
            output_response_exp
        )
    )
    ret = local_agent.generate()
    if "```" in ret:
        ret = ret.split("```")[-2].lstrip("json")
    ret = response_format.model_validate_json(ret).model_dump()
    if not with_subqs:
        return {"_": ret}
    return ret
```

Let's also update our system message to explain to better explain to the agent how it should reason about this new output structure.

```python
output_response_explanations = dict()
output_response_explanations["with_subqs"] = "For each sub-question {subquestions}, you should populate the `reasoning` field with your general thoughts on each individual mark identified in the markscheme, and also a decision as to whether each of these mark should be awarded. These marks are not necessarily cumulative with regards to the marks to award, and some may be irrelevant given the student's approach or answer, in which case just respond `False` for the `should_award` field. Finally, you should put the total number of marks to award for each sub-question in the corresponding `marks` field."
output_response_explanations["without_subqs"] = "You should populate the `reasoning` field with your general thoughts on each individual mark identified in the markscheme, and also a decision as to whether each of these mark should be awarded. These marks are not necessarily cumulative with regards to the marks to award, and some may be irrelevant given the student's approach or answer, in which case just respond `False` for the `should_award` field. Finally, you should put the total number of marks to award in the `marks` field."
```

We also need to update `evaluate`, given some of the other changes.

```python
@unify.log
def evaluate(
    question,
    sub_questions,
    student_answer,
    available_marks_total,
    markscheme,
    correct_marks,
    per_question_breakdown,
    _system_message,
):
    pred_marks = call_agent(
        _system_message, question, sub_questions, markscheme, student_answer,
        available_marks_total
    )
    pred_marks_total = sum([v["marks"] for v in pred_marks.values()])
    diff = {
        k: vcor["marks"] - vpred["marks"] for (k, vcor), (_, vpred) in
        zip(correct_marks.items(), pred_marks.items())
    }
    error = {k: abs(v) for k, v in diff.items()}
    diff_total = sum(diff.values())
    error_total = sum(error.values())
    per_question_breakdown = {
        k: {
            **per_question_breakdown[k],
            "predicted_marks": pm,
            "diff": d
        } for (k, pqb), pm, d in zip(
            per_question_breakdown.items(),
            pred_marks.values(),
            diff.values()
        )
    }
    return error
```

### 🧪 Rreun Tests

```python
with unify.Experiment("mark_type_reasoning"), unify.Params(
    system_message=system_message,
    model=agent.model,
    dataset="dataset_10"
):
    unify.map(
        evaluate,
        [
             dict(**d.entries, _system_message=system_message)
             for d in test_set_10
        ],
        name="Evals"
    )
```

The failure modes are **still entirely unchanged!** `o3-mini` is certainly very stubborn about it's decision for these questions.

### 🔍 Still Ignoring Mark Types

Let's see what affect our new output format had on the nature of the agent's responses, if any.

Considering **Example 207**, the agent still failed to award `SC1` for the student's answer `1/3, 0.34, 3.5%`, despite the markscheme explicitly stating `SC1 for 1/3, 0.34, 3.5%`. The agent's explicit `thoughts` about `SC1` were:

> 🤖 No special case credit is applicable here since the order is incorrect and no alternative acceptable method is demonstrated.

This is a pretty fluffy and empty statement. Despite o3-mini being a multi-step reasoning model, perhaps we're *still* asking the agent to consider too many things at once.

Enforcing the agent to consider one mark at a time might rectify this lack of attention to detail.

**Example 132** is even more difficult, where the agent not only needs to consider each mark, but it also has six different sub-questions to reason about, each with their own set of available marks and mark types.

Let's see if using a separate LLM call per sub-question improves the performance on **Example 132**.

### 🔀 Queries per Subquestion

Firstly, let's create a new system prompt for our agent, which will reason about one-subquestion at a time.

```python
system_message = """
Your task is to award a suitable number of marks for a student's answer to a question, from 0 up to a maximum of {available_marks} marks.

The general marking guidelines (relevant for all questions) are as follows:

{general_guidelines}


The question you need to mark is:

{question}


Their answer is:

{answer}


The markscheme is:

{markscheme}


{output_response_explanation}
""".replace(
    "{general_guidelines}",
    general_guidelines
)
```

Given the changes, we can also remove the `output_response_explanations` dict, and relace it with a single `output_response_explanation` string variable, given that the agent no longer needs to output responses for multiple sub-questions in a single response.

```python
output_response_explanation = "You should populate the `reasoning` field with your general thoughts on each individual mark identified in the markscheme, and also a decision as to whether each of these mark should be awarded. These marks are not necessarily cumulative with regards to the marks to award, and some may be irrelevant given the student's approach or answer, in which case just respond `False` for the `should_award` field. Finally, you should put the total number of marks to award in the `marks` field."
```

Let's update `call_agent` to map each subquestion to a unique LLM call, and let's also add a `unify.traced` decorator so we're able to capture each individual LLM call in the overall trace.

```python
@unify.traced
def call_agent(example_id, system_msg, sub_questions, markscheme, answer, available_marks):
    agents = {k: agent.copy() for k in markscheme.keys()}
    with_subqs = len(markscheme) > 1
    response_formats = {
        k: create_marks_and_reasoning_format(
            [itm[0] for itm in parse_marks_from_markscheme(v)]
        ) for k, v in markscheme.items()
    }
    [
        agnt.set_response_format(rf)
        for agnt, rf in zip(
            agents.values(), response_formats.values()
        )
    ]
    markscheme = {
        k: update_markscheme(v) for k, v in markscheme.items()
    }
    for k in markscheme.keys():
        agents[k].set_system_message(
            system_msg.replace(
                "{question}", sub_questions[k]
            ).replace(
                "{markscheme}", markscheme[k]
            ).replace(
                "{answer}", answer[k]
            ).replace(
                "{available_marks}",
                str(available_marks[k.replace("_", "total")])
            ).replace(
                "{output_response_explanation}",
                output_response_explanation
            )
        )
    rets = unify.map(
        lambda a: a.generate(),
        list(agents.values()),
        name=f"Evals[{example_id}]->SubQAgent"
    )
    rets = [
        ret.split("```")[-2].lstrip("json")
        if "```" in ret else ret
        for ret in rets
    ]
    rets = {
        k: response_formats[k].model_validate_json(ret).model_dump()
        for k, ret in zip(markscheme.keys(), rets)
    }
    return rets
```

Let's also update `evaluate` to pass the updated parameters to `call_agent`.

```python
@unify.log
def evaluate(
    example_id,
    question,
    sub_questions,
    student_answer,
    available_marks,
    markscheme,
    correct_marks,
    per_question_breakdown,
    _system_message,
):
    pred_marks = call_agent(
        example_id, _system_message, sub_questions, markscheme, student_answer,
        available_marks
    )
    pred_marks_total = sum([v["marks"] for v in pred_marks.values()])
    diff = {
        k: vcor["marks"] - vpred["marks"] for (k, vcor), (_, vpred) in
        zip(correct_marks.items(), pred_marks.items())
    }
    error = {k: abs(v) for k, v in diff.items()}
    diff_total = sum(diff.values())
    error_total = sum(error.values())
    per_question_breakdown = {
        k: {
            **per_question_breakdown[k],
            "predicted_marks": pm,
            "diff": d
        } for (k, pqb), pm, d in zip(
            per_question_breakdown.items(),
            pred_marks.values(),
            diff.values()
        )
    }
    return error
```

### 🧪 Rreun Tests

```python
with unify.Experiment("queries_per_subquestion"), unify.Params(
    system_message=system_message,
    model=agent.model,
    dataset="dataset_10"
):
    unify.map(
        evaluate,
        [
             dict(**d.entries, _system_message=system_message)
             for d in test_set_10
        ],
        name="Evals"
    )
```
