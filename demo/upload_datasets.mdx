---
title: 'Upload Datasets'
---

## Users

In the previous section we built a usage dashboard,
showing the traffic coming from 100 active users on the platform üìà

Let's now create a "Users" dataset to store all of the user data in one place üóÇÔ∏è

We have exported these details to
[users.json](https://github.com/unifyai/demos/blob/main/marking_assistant/data/users.json).
We can easily upload this to the platform via the Python client.

Let's add imports,

```python
import os
import wget
import json
import unify
```

activate the project,

```python
unify.activate("MarkingAssistant")
```

download the data,

```python
if not os.path.exists("users.json"):
    wget.download(
        "https://github.com/unifyai/demos/"
        "raw/refs/heads/main/marking_assistant/"
        "data/users.json"
    )
```

and read the data.

```python
with open("users.json", "r") as f:
    users = json.load(f)
```

We can then create a dataset like so:

```python
users_dataset = unify.Dataset(users, name="Users")
```

It's good practice to use `.sync()` when uploading,
as this performs a bi-directional sync,
and uploads/downloads data to achieve the superset both locally and upstream.

```python
users_dataset.sync()
```

In this case the dataset did not exist upstream,
and so `.sync()` was equivalent to calling `.upload()`.

Let's now create a new `Dataset` Tab in our interface,
and set the context of the *entire tab* to `Datasets`
such that all tables will only have access to this context.
The only datasets, `Users`, is then loaded into the table automatically.

<Accordion title="Expand">
<img class="dark-light" width="100%" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/refs/heads/main/img/externally_linked/docs/demo_datasets_create_tab.gif"/>
click image to maximize
</Accordion>

We can sort the data alphabetically if we want the data to be more structured.

GIF

We can also search for any student in the search bar.

GIF

## Test Set

Before we deploy an agent to mark the user answers in production,
we need to be able to evaluate the agent performance.
This is what a test set is for,
matching agent *inputs* with desired agent *outputs*.

Let's assume we have a dataset of questions,
answers and the *correct* number of marks to award for each answer,
that an expert marker has provided,
alongside their rationale for awarding this number of marks.

In our case, this was synthetically generated by OpenAI's o1 via
[this script](https://github.com/unifyai/demos/blob/main/ai_tutor/generate_data.py),
but for the sake of example, we can assume it was generated by expert human markers.

The data was then organized into a test set using
[this script](https://github.com/unifyai/demos/blob/main/marking_assistant/generate_test_dataset.py),
and the resultant data is saved in [test_set.json](https://github.com/unifyai/demos/blob/main/marking_assistant/data/test_set.json).


As before, lets download the data,

```python
if not os.path.exists("users.json"):
    wget.download(
        "https://github.com/unifyai/demos/"
        "raw/refs/heads/main/marking_assistant/"
        "data/test_set.json"
    )
```

read the data,

```python
with open("test_set.json", "r") as f:
    test_set = json.load(f)
```

create a dataset,

```python
test_set = unify.Dataset(test_set, name="TestSet")
```

and upload it into the platform.

```python
test_set.sync()
```

Let's see if the test set has been created correctly.

GIFs