---
title: 'Upload Datasets'
---

## Users

In the previous section we built a usage dashboard,
showing the traffic coming from 100 active users on the platform 📈

Let's create a "Users" dataset to store all of the user data in one place 🗂️

We have exported these details to a `users.json` file,
which can be found [here](https://github.com/unifyai/demos/blob/main/marking_assistant/data/users.json).

We can easily upload this to the platform via the Python client.

```python
import os
import wget
import json

import unify
unify.activate("MarkingAssistantTest")

if not os.path.exists("users.json"):
    wget.download(
        "https://github.com/unifyai/demos/raw/refs/heads/main/marking_assistant/data/users.json"
    )
with open("users.json", "r") as f:
    users = json.load(f)

users_dataset = unify.Dataset(users, name="Users")
users_dataset.sync() # bi-directional sync
```

Let's now create a new `Dataset` Tab in our interface to view our new `Users` dataset.

GIF

## Test Set

Before we deploy an agent to mark the user answers in production,
we need to be able to evaluate the agent performance.
This is what a test set is for,
matching agent inputs with desired agent outputs.
Let's assume we have an expert dataset of questions,
answers and the *correct* number of marks to award for each answer.
In our case, this was synthetically generated by OpenAI's o1 via [this script](),
but we can assume it was generated by expert human markers.

Let's upload this test set into the platform.

```python
with open("labelled_data.json", "r") as f:
    labelled_data = json.load(f)
unify.upload_dataset(labelled_data, name="TestSet")
```

