---
title: 'Upload Datasets'
---

In the previous section we built a usage dashboard,
showing the traffic coming from 100 active users on the platform üìà

Let's now create some datasets so we can keep tabs on our users
and also have some ground truth data in order to optimize our LLM agent in the next section.

# Users

So, let's first create the "Users" dataset to store all of the user data in one place üóÇÔ∏è

<div
  style={{
    position: 'relative',
    paddingBottom: '56.25%', // 16:9 ratio
    height: 0,
  }}
>
  <iframe
    style={{
      position: 'absolute',
      top: 0,
      left: 0,
      width: '100%',
      height: '100%',
      border: 'none'
    }}
    src="https://www.youtube.com/embed/94JjxO5LZtI"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowFullScreen
  />
</div>

## Upload

We have exported these details to
[users.json](https://github.com/unifyai/demos/blob/main/marking_assistant/data/users.json).
We can easily upload this to the platform via the Python client.

Let's add imports,

```python
import os
import wget
import json
import unify
```

activate the project,

```python
unify.activate("MarkingAssistant")
```

download the data,

```python
if not os.path.exists("users.json"):
    wget.download(
        "https://github.com/unifyai/demos/"
        "raw/refs/heads/main/marking_assistant/"
        "data/users.json"
    )
```

and read the data.

```python
with open("users.json", "r") as f:
    users = json.load(f)
```

We can then create a dataset like so:

```python
users_dataset = unify.Dataset(users, name="Users")
```

It's good practice to use `.sync()` when uploading,
as this performs a bi-directional sync,
and uploads/downloads data to achieve the superset both locally and upstream.

```python
users_dataset.sync()
```

In this case the dataset did not exist upstream,
and so `.sync()` was equivalent to calling `.upload()`.

The full script for uploading this users dataset (and the ones mentioned below) can be found
[here](https://github.com/unifyai/demos/blob/main/marking_assistant/upload_datasets.py).

## Analyze

Let's now create a new `Dataset` Tab in our interface,
and set the context of the *entire tab* to `Datasets`
such that all tables will only have access to this context.
The only datasets, `Users`, is then loaded into the table automatically.

<Accordion title="Expand">
<img class="dark-light" width="100%" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/refs/heads/main/img/externally_linked/docs/demo_datasets_create_tab.gif"/>
click image to maximize
</Accordion>

### Sort Surnames

We can sort the data alphabetically if we want the data to be more structured.

<Accordion title="Expand">
<img class="dark-light" width="100%" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/refs/heads/main/img/externally_linked/docs/demo_datasets_sort_users.gif"/>
click image to maximize
</Accordion>

### Search for Students

We can also search for any student in the search bar.

<Accordion title="Expand">
<img class="dark-light" width="100%" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/refs/heads/main/img/externally_linked/docs/demo_datasets_global_search.gif"/>
click image to maximize
</Accordion>

# Test Set

Before we deploy an agent to mark the user answers in production,
we need to be able to evaluate the agent performance.
This is what a test set is for,
matching agent *inputs* with desired agent *outputs*.

<div
  style={{
    position: 'relative',
    paddingBottom: '56.25%', // 16:9 ratio
    height: 0,
  }}
>
  <iframe
    style={{
      position: 'absolute',
      top: 0,
      left: 0,
      width: '100%',
      height: '100%',
      border: 'none'
    }}
    src="https://www.youtube.com/embed/YmScUwJ1ulw"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowFullScreen
  />
</div>

## Upload

Let's assume we have a dataset of questions,
answers and the *correct* number of marks to award for each answer,
that an expert marker has provided,
alongside their rationale for awarding this number of marks.

In our case, this was synthetically generated by OpenAI's o1 via
[this script](https://github.com/unifyai/demos/blob/main/ai_tutor/generate_data.py),
but for the sake of example, we can assume it was generated by expert human markers.

The data was then organized into a test set using
[this script](https://github.com/unifyai/demos/blob/main/marking_assistant/generate_test_dataset.py),
and the resultant data is saved in [test_set.json](https://github.com/unifyai/demos/blob/main/marking_assistant/data/test_set.json).

### Full Dataset

As before, lets download the data,

```python
if not os.path.exists("users.json"):
    wget.download(
        "https://github.com/unifyai/demos/"
        "raw/refs/heads/main/marking_assistant/"
        "data/test_set.json"
    )
```

read the data,

```python
with open("test_set.json", "r") as f:
    test_set = json.load(f)
```

create a dataset,

```python
test_set = unify.Dataset(test_set, name="TestSet")
```

and upload it into the platform.

```python
test_set.sync()
```

### Sub-Datasets

Whilst we're improving our LLM agent (next section),
we won't necessarily want to test against **all** 321 examples **every time**.
This would be both very costly and very time consuming,
and needlessly wasteful early on,
when a handful of examples will suffice to point us in the right direction.

Therefore, let's create some subsets of the full test dataset.
Let's start with 10 examples,
and then double up to 160 (almost half the full size).
The test set has already been shuffled,
so we can simply take increasing slices starting from the beginning.

```python
for size in [10, 20, 40, 80, 160]:
    test_set[0:size].set_name(f"TestSet{size}").sync()
```

As mentioned above, the full script for uploading all of these datasets into the platform can be found
[here](https://github.com/unifyai/demos/blob/main/marking_assistant/upload_datasets.py).

## Analyze

Let's now analyze the test datasets, to verify everything is as it should be  üîç

### Check Sub-Datasets

Let's take a look at our different dataset slices.

Each sub-dataset contains the **same** logs as the main dataset,
and as every other overlapping sub-dataset.
For example,
any in-place updates to the logs will be reflected in **all** datasets.

<Accordion title="Expand">
<img class="dark-light" width="100%" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/refs/heads/main/img/externally_linked/docs/demo_datasets_sub_test_sets.gif"/>
click image to maximize
</Accordion>

### Verify All Data is Present

Let's verify that all of the data is present, as expected.

#### Number of Papers

We should have a total of three papers, as per this
[original PDF](https://www.ocr.org.uk/Images/169000-foundation-tier-sample-assessment-materials.pdf),
and as per the parsed representation we extracted
[here](https://github.com/unifyai/demos/tree/main/marking_assistant/data/parsed/GCSE_(9%E2%80%931)_Mathematics).
We can confirm this by grouping by the papers.

<Accordion title="Expand">
<img class="dark-light" width="100%" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/refs/heads/main/img/externally_linked/docs/group_papers.gif"/>
click image to maximize
</Accordion>

#### Number of Questions

Each paper should have a set number of questions.
Checking [the original PDF](https://www.ocr.org.uk/Images/169000-foundation-tier-sample-assessment-materials.pdf),
we can see that:

- Paper1 -> 21 Questions
- Paper 2 -> 19 Questions
- Paper 3 -> 19 Questions

However, questions are also **omitted** from the test set if:

- they involve an image as **part of the question**, (they are [not text-only](https://github.com/unifyai/demos/blob/main/marking_assistant/helpers.py#L125))
- the question was not `correctly_parsed` (example [parsed paper](https://github.com/unifyai/demos/blob/main/marking_assistant/data/parsed/GCSE_(9%E2%80%931)_Mathematics/J560_01_Paper_1_(Foundation_Tier)_Sample_Question_Paper/paper/parsed.json))
- the question markscheme was not `correctly_parsed` (example [parsed markscheme](https://github.com/unifyai/demos/blob/main/marking_assistant/data/parsed/GCSE_(9%E2%80%931)_Mathematics/J560_01_Paper_1_(Foundation_Tier)_Sample_Question_Paper/markscheme/parsed.json))

Let's check how many questions were used from each of the three papers.
We can simply *also* group by the question number,
and then unfold these nested groups.

<Accordion title="Expand">
<img class="dark-light" width="100%" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/refs/heads/main/img/externally_linked/docs/group_questions.gif"/>
click image to maximize
</Accordion>

As expected,
we can see that not all of the questions are replicated in the test set.
Upon further inspection,
we can see that the following questions are all ommitted due to
the inclusion of necessary images as part of the question (`"text-only": false`):

- [Paper1->Question12](https://github.com/unifyai/demos/blob/main/marking_assistant/data/parsed/GCSE_(9%E2%80%931)_Mathematics/J560_01_Paper_1_(Foundation_Tier)_Sample_Question_Paper/paper/parsed.json#L131)
- [Paper1->Question13](https://github.com/unifyai/demos/blob/main/marking_assistant/data/parsed/GCSE_(9%E2%80%931)_Mathematics/J560_01_Paper_1_(Foundation_Tier)_Sample_Question_Paper/paper/parsed.json#L146)
- [Paper1->Question16](https://github.com/unifyai/demos/blob/main/marking_assistant/data/parsed/GCSE_(9%E2%80%931)_Mathematics/J560_01_Paper_1_(Foundation_Tier)_Sample_Question_Paper/paper/parsed.json#L183)
- [Paper3->Question2](https://github.com/unifyai/demos/blob/main/marking_assistant/data/parsed/GCSE_(9%E2%80%931)_Mathematics/J560_03_Paper_3_(Foundation_Tier)_Sample_Question_Paper/paper/parsed.json#L17)

#### Number of Target Answers

Finally, each question should have `N+1` candidate answers,
where `N` is the number of available marks: `[0, 1, ..., N]`.
Let's verify this is correct,
by adding the `available_marks_total` column,
and verifying that the group sizes are all equal to `available_marks_total + 1`.

<Accordion title="Expand">
<img class="dark-light" width="100%" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/refs/heads/main/img/externally_linked/docs/unfold_available_marks_total.gif"/>
click image to maximize
</Accordion>

It seems as though the data is all formatted as we expected it to be,
and the 321 ground truth examples do indeed exhaustively fill all possible marks
for all the valid (text-only) questions across all papers ‚úÖ

That's it, we're now ready to implement our LLM agent,
and start iterating to improve the performance! üîÅ