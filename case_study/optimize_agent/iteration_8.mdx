---
title: 'Iteration 8'
---

The full script for running this iteration can be found
[here](https://github.com/unifyai/demos/blob/main/marking_assistant/optimize_agent/iteration_8.py).

## 🔍 Lack of Global Context

These updates have actually regressed the overall performance, with us now having a mean error of `0.5`.

Maybe the purely local reasoning has some shortcomings. Let's focus on one of the **new regressions**, to understand why our latest change has distrupted the agent where it was previously very consistently correct.

**Example 20 (b)**

❓ Question [1 Mark]:

> Explain why he might not want to use Account A.

📝 Student's Answer:

> He might not want to use Account A because the bank might keep the money forever and he could lose it.

☑️ Markscheme:

> He may not want to leave it there for 3 years. Accept any valid reason.

✅ Correct Marks [0/1] Rationale:

> The markscheme awards 1 mark for a valid reason such as not wanting to lock away funds for 3 years. Here, the stated reason is irrelevant and does not accurately reflect the restrictions of Account A.

🤖 Predicted Marks [1/1] ❌ Rationale:

> The mark scheme awards 1 mark for any valid reason why the person might not want to leave the money in Account A for 3 years. The student's answer, although phrasing it as the bank might keep the money forever and thus he could lose it, is a valid reason to be cautious, even though it doesn't mention the explicit time period of 3 years. This interpretation sufficiently aligns with the mark scheme's requirement of a valid reason. Therefore, the mark is awarded.

Clearly, the agent is now taking some liberties with what constitutes a "valid reason". It's unclear why the agent is only making these mistakes now.

Let's look at some of the previous justifications for **not** awarding the mark, from the previous runs.

🤖 Predicted Marks [x/1] Rationales:

> `simple_agent` [0/1] ✅: The answer should explain that although Account A gives slightly more money, Derrick might not want this account because his money is tied up for three years (i.e. it has poor liquidity). The student's explanation about the bank "keeping the money forever" is not an acceptable financial rationale.

> `add_markscheme` [0/1] ✅: The explanation provided is not a valid reason and does not address the restriction of no withdrawals for three years. Therefore, no mark can be awarded for this part either.

> `add_marking_guidelines` [0/1] ✅: The explanation given ("because the bank might keep the money forever and he could lose it") does not correctly address why he might not want to use Account A (namely, that his money is locked away for 3 years with no withdrawals allowed).

> `add_structured_output` [0/1] ✅: The answer needed to explain why Account A might not be desirable, which is because funds are locked in for three years and withdrawals are not allowed until the end of the period. The student's answer states that the bank might keep the money forever and he could lose it, which does not address the actual restriction (lack of liquidity). This is an irrelevant or incorrect reason and does not meet the mark scheme.

> `align_context` [0/1] ✅: The mark scheme accepts any valid reason such as not being able to withdraw the money because it is locked in for 3 years. The candidate's answer – that the bank might 'keep the money forever' and he could lose it – is not a valid explanation for the funds being inaccessible. It does not clearly express that his money is tied up for 3 years. Therefore, this explanation does not meet the requirement and no mark should be awarded.

> `align_guidelines_and_clarify_reasoning` [0/1] ✅: The answer to part (b) should explain that Account A requires the money to be locked in for three years which might be a disadvantage if immediate access is desired. The student's explanation – that the bank might 'keep the money forever' – does not address the real issue of the restriction on withdrawals. As the explanation provided is not a valid reason as per the mark scheme, no marks should be awarded for this part.

> `mark_type_reasoning` [0/1] ✅: The explanation provided does not address the key issue mentioned in the mark scheme (the restriction of no withdrawals for 3 years) and instead gives a non sequitur reason that the bank might 'keep the money forever.' This does not meet the required rationale, so no mark is awarded.

Perhaps preventing the agent from having access to the full question prevents it from using "common sense" and realizing how "silly" the proposed answer is, in light of the overall question and the information provided to the student.

Maybe strict adherence to the markscheme alone without the full context is prohibitive.

Let's update our per-subquestion system prompts to also fully include the **preceeding** sub-questions,
their markschemes,
and their answers.
It's unlikely that the context of a **later** question will assists with the marking of an earlier question,
and we still want to try and keep the agent as focused as possible on the relevant information.

## 🔀 Include Preceeding Context

Let's first update the system prompt,
re-introducing the placeholder for the aligned subquestions,
markschemes and answers,
this time calling it `{prior_context}`,
which will only be included when sub-questions are present.
Let's also include the full question.

``` {2,9,13,15-20,25}
system_message = """
Your task is to award a suitable number of marks for a student's answer to question {subq}, from 0 up to a maximum of {available_marks} marks.

The general marking guidelines (relevant for all questions) are as follows:

{general_guidelines}


The *overall* question is:

{question}

{prior_context}

The specific question you need to mark is:

{subquestion}


Their answer to this specific question is:

{answer}


The markscheme for this specific question is:

{markscheme}


{output_response_explanation}
""".replace(
    "{general_guidelines}",
    general_guidelines,
)
```

Let's also add a general explanation for the prior context, in cases where it is included.

```
prior_context_exp = """
All of the *preceeding* sub-questions, their specific markschemes and the student's answers are as follows:
"""
```

Let's now update `call_agent` to pass in the required information.

```python {5-6,13,34,37-40,42-45,64-84,87-88}
@unify.traced
def call_agent(
    example_id,
    system_msg,
    question_num,
    question,
    sub_questions,
    markscheme,
    answer,
    available_marks,
):
    agents = {k: agent.copy() for k in markscheme.keys()}
    with_subqs = len(markscheme) > 1
    response_formats = {
        k: create_marks_and_reasoning_format(
            [
                itm[0]
                for itm in parse_marks_from_markscheme(f"_{k}" if k != "_" else "", v)
            ],
        )
        for k, v in markscheme.items()
    }
    [
        agnt.set_response_format(rf)
        for agnt, rf in zip(
            agents.values(),
            response_formats.values(),
        )
    ]
    markscheme = {
        k: update_markscheme(f"_{k}" if k != "_" else "", v)
        for k, v in markscheme.items()
    }
    for i, k in enumerate(markscheme.keys()):
        agents[k].set_system_message(
            system_msg.replace(
                "{subq}",
                k.replace("_", str(question_num)),
            )
            .replace(
                "{question}",
                textwrap.indent(question, " " * 4),
            )
            .replace(
                "{subquestion}",
                textwrap.indent(sub_questions[k], " " * 4),
            )
            .replace(
                "{markscheme}",
                textwrap.indent(markscheme[k], " " * 4),
            )
            .replace(
                "{answer}",
                textwrap.indent(answer[k], " " * 4),
            )
            .replace(
                "{available_marks}",
                str(available_marks[k.replace("_", "total")]),
            )
            .replace(
                "{output_response_explanation}",
                output_response_explanation,
            )
            .replace(
                "{prior_context}",
                (
                    (
                        prior_context_exp
                        + pretty_print_dict(
                            {
                                k: {
                                    "sub-question": sub_questions[k],
                                    "markscheme": markscheme[k],
                                    "answer": answer[k],
                                }
                                for k in list(sub_questions.keys())[0:i]
                            },
                            indent=4,
                        )
                    )
                    if with_subqs and i > 0
                    else ""
                ),
            ),
        )
    rets = unify.map(
        lambda a: a.generate(tags=[k]),
        list(agents.values()),
        name=f"Evals[{example_id}]->SubQAgent",
    )
    rets = [
        ret.split("```")[-2].lstrip("json") if "```" in ret else ret for ret in rets
    ]
    rets = {
        k: response_formats[k].model_validate_json(ret).model_dump()
        for k, ret in zip(markscheme.keys(), rets)
    }
    return rets
```

Finally, let's update `evaluate` accordingly.

```python {4-5,17-18}
@unify.log
def evaluate(
    example_id,
    question_num,
    question,
    sub_questions,
    student_answer,
    available_marks,
    markscheme,
    correct_marks,
    per_question_breakdown,
    _system_message,
):
    pred_marks = call_agent(
        example_id,
        _system_message,
        question_num,
        question,
        sub_questions,
        markscheme,
        student_answer,
        available_marks,
    )
    pred_marks_total = sum([v["marks"] for v in pred_marks.values()])
    diff = {
        k: vcor["marks"] - vpred["marks"]
        for (k, vcor), (_, vpred) in zip(correct_marks.items(), pred_marks.items())
    }
    error = {k: abs(v) for k, v in diff.items()}
    diff_total = sum(diff.values())
    error_total = sum(error.values())
    per_question_breakdown = {
        k: {
            **per_question_breakdown[k],
            "predicted_marks": pm,
            "diff": d,
        }
        for (k, pqb), pm, d in zip(
            per_question_breakdown.items(),
            pred_marks.values(),
            diff.values(),
        )
    }
    return error_total
```

## 🧪 Rerun Tests

```python {2}
with unify.Experiment(
    "with_preceeding_context",
    overwrite=True,
), unify.Params(
    system_message=system_message,
    dataset="TestSet10",
    source=unify.get_source(),
):
    unify.map(
        evaluate,
        [dict(**d.entries, _system_message=system_message) for d in test_set_10],
        name="Evals",
    )
```

Great, so we've fixed the new regressions, but again we're back at the same three failures, failing for the same reason.

Let's take a look at the traces, to ensure that the system message template has been implemented correctly,
and each LLM call has the template variables in the system message populated correctly.

<div
  style={{
    position: 'relative',
    paddingBottom: '56.25%', // 16:9 ratio
    height: 0,
  }}
>
  <iframe
    style={{
      position: 'absolute',
      top: 0,
      left: 0,
      width: '100%',
      height: '100%',
      border: 'none'
    }}
    src="https://www.youtube.com/embed/{placeholder}"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowFullScreen
  />
</div>

It seems as though everything was implemented correctly,
and the per-LLM system messages look good ✅

Again, let's explore what's going wrong in the next iteration 🔁