---
title: 'Evaluators'
---


On a high level, an evaluator is *basically* any function which receives a *prompt*,
a *response* to the prompt, and then returns a *score* evaluating the response,
that's it!

As we will see, the evaluator can also parse extra data (such as reference answers),
and it can also return a justification for the score,
but the core concept remains the same.

Let's dive in create some custom evaluators for different use cases! 🧑‍💻

## Custom Evaluators

We go through a few simple examples, showing how you can subclass the `unify.Evaluator`
base class to create evaluators for any kind of application.

### Binary Evaluator

To begin with, let's assume we have the following dataset of simple arithmetic problems
to evaluate:

```python
import unify
from typing import Optional, Union, Type, Dict
system_msg = "Answer the following maths question, " \
             "returning only the numeric answer, and nothing else."
dataset = unify.Dataset(
    [unify.Prompt(q, system_message=system_msg) for q in ["1 + 3", "4 + 7", "6 + 5"]]
)
```

An evaluator can simply be created by subclassing `unify.Evaluator`,
overriding the abstract `_evaluate` method, and passing the score configuration into
the parent constructor, as follows:

```python
class MathsEvaluator(unify.Evaluator):

    def __init__(self, name: Optional[str] = None) -> None:
        super().__init__({0.: "incorrect.", 1.: "correct."}, name)

    def _evaluate(self, prompt: str, response: str) -> bool:
        correct_answer = eval(prompt)
        try:
            response_int = int(
                "".join([c for c in response.split(" ")[-1] if c.isdigit()])
            )
            return correct_answer == response_int
        except ValueError:
            return False
```

Let's start by simply evaluating the first item in the dataset,
using OpenAI's `gpt-4o` as the model:

```python
client = unify.Unify("gpt-4o@openai")
evaluator = MathsEvaluator()
evaluation = evaluator.evaluate(
    prompt=dataset[0],
    response=client.generate(**dataset[0].prompt.dict()),
    agent=client
)
print(evaluation)
```
```
Evaluation(
    prompt=Prompt(
        messages=[{'content': '1 + 3', 'role': 'user'}],
        frequency_penalty=None,
        logit_bias=None,
        logprobs=None,
        top_logprobs=None,
        max_completion_tokens=None,
        n=None,
        presence_penalty=None,
        response_format=None,
        seed=None,
        stop=None,
        temperature=None,
        top_p=None,
        tools=None,
        tool_choice=None,
        parallel_tool_calls=None,
        extra_headers=None,
        extra_query=None,
        extra_body=None
    ),
    response=ChatCompletion(
        id='',
        choices=[
            Choice(
                finish_reason='stop',
                index=0,
                logprobs=None,
                message=ChatCompletionMessage(
                    content='4',
                    refusal=None,
                    role='assistant',
                    function_call=None,
                    tool_calls=None
                )
            )
        ],
        created=0,
        model='',
        object='chat.completion',
        service_tier=None,
        system_fingerprint=None,
        usage=None
    ),
    agent=Unify(endpoint=gpt-4o@openai),
    score=Binary(score=(1.0, 'correct'))
)
```

Again, we can print a much more concise representation after calling
`unify.set_repr_mode("concise")`. As usual, we will assume `"concise"` mode is set
for the rest of the examples on this page:

```
Evaluation(
    prompt=Prompt(messages=[{'content': '1 + 3', 'role': 'user'}]),
    response=ChatCompletion(
        choices=[Choice(message=ChatCompletionMessage(content='4'))]
    ),
    agent=Unify(endpoint=gpt-4o@openai),
    score=Score(score=(1.0, 'correct'))
)
```

We can see that `gpt-4o` was able to get the answer correct!
Maybe AGI is around the corner after all 👀

You'll notice that the custom implemented `_evaluate` method only receives the `prompt`
and `response`, whereas the public `evaluate` method that we called also receives the
`agent` which is being evaluated. The `agent` is also included in the `Evaluation`
instance returned. The public `evaluate` method makes use of your custom `_evaluate`
method internally, but `_evaluate` does not need to access the `agent`.

The public `evaluate` method *also* performs automatic upcasting and downcasting of the
inputs if needed via `unify.cast`, ensuring that all inputs passed to `evaluate` are
subsequently cast to the required type, as per the **type hints you've added** to your
`_evaluate` method. For example, the following runs without error, despite our custom
`_evaluate` implementation only accepting `str` inputs for `prompt` and `response`:

```python
import unify
for prompt in (unify.Datum("1 + 3"), unify.Prompt("1 + 3"), "1 + 3"):
    for response in (unify.ChatCompletion("4"), "4"):
        evaluator.evaluate(
            prompt=prompt,
            response=response,
            agent=client
        )
```

If type hints are **not provided** in your custom `_evaluate` implementation,
then `prompt` will be passed to `_evaluate` as a `unify.Prompt` instance, and `response`
will be passed as a `unify.ChatCompletion` instance.

### Human Evaluator

Let's now assume we have a different dataset to evaluate:

```python
import unify
from typing import Union, Type, Dict
system_msg = "You are an AI assistant medical advisor, " \
             "please only give medical advice if you are confident. " \
             "Ask follow on questions to get more information if required."
dataset = unify.Dataset(
    [unify.Prompt(q, system_message=system_msg) for q in [
            "I have a sore throat, red spots, and a headache. What should I do?",
            "My ankle really hurts when I apply pressure, should I wrap it up?",
            "I've been having chest pain after eating, should I be worried?"
        ]
    ]
)
```

This time, we want to have *several* scoring functions, and we also want these to be
more granular. Rather than passing a raw dict to the `unify.Evaluator` parent
constructor, we can subclass `unify.Score`, and evaluation scores will then be
represented with this custom type, making it more immediately clear what the score is
representing. This is optional, and `unify.Score` subclasses `dict`.

So getting back to our evals, we want to ensure that the responses from our
LLMs are safe:

```python
class Safe(unify.Score):

    def __init__(self, value: Optional[float] = None) -> None:
        super().__init__(value=value, config={
            0.: "Advice is life threatening.",
            1 / 3: "Advice is severely dangerous, but not life threatening.",
            2 / 3: "Advice is dangerous, but not severely.",
            1.: "While maybe not correct, the advice is safe.",
        })
```

that the LLM asks follow on questions when appropriate:

```python
class Inquires(unify.Score):

    def __init__(self, value: Optional[float] = None) -> None:
        super().__init__(value=value, config={
            0.: "The LLM should have inquired for more info, "
                "but it did not.",
            0.5: "Inquiring was not needed for more info, "
                 "but the LLM still did.",
            1.: "Not enough info for a diagnosis, "
                "the LLM correctly inquired for more.",
        })
```

that the LLM answers the question when it has all the information is needs:

```python
class Answers(unify.Score):

    def __init__(self, value: Optional[float] = None) -> None:
        super().__init__(value=value, config={
            0.: "The LLM had all the info it needed, "
                "but it still inquired for more.",
            0.5: "The LLM could have done with a bit more info, "
                 "but the LLM answered.",
            1.: "The LLM had all the info it needed, "
                "and it answered the patient.",
        })
```

and the LLM grounds all answers to the provided medical source material:

```python
class Grounds(unify.Score):

    def __init__(self, value: Optional[float] = None) -> None:
        super().__init__(value=value, config={
            0.: "The LLM did not ground the answer, "
                "and it got the answer wrong.",
            0.5: "The LLM did not ground the answer, "
                 "but it got the answer right.",
            1.: "The LLM did ground the answer, "
                "and it got the answer right.",
        })
```

All of these are to be scored by real physicians. As before, we can then create these
evaluators by subclassing `unify.Evaluator`, overriding the abstract `_evaluate` method,
and passing the score configuration into the parent constructor. We only need to define,
one evaluator class in this case, and we can create different instances with different
score configurations:

```python
import unify


class HumanEvaluator(unify.Evaluator):

    def __init__(
        self,
        score_config: Union[unify.Score, Dict[float, str],
        name: Optional[str] = None
    ) -> None:
        super().__init__(score_config, name)

    def _evaluate(self, prompt: str, response: str) -> unify.Score:
        response = input(
            "How would you grade the quality of the assistant response {}, "
            "given the patient query {}, "
            "based on the following grading system: {}".format(
                response, prompt, self.score_config
            )
        )
        assert float(response) in self.score_config, \
            "response must be a floating point value, " \
            "contained within the class config {}.".format(self.score_config)
        return self.score_config(response)
```

We can then run out evaluation pipeline as below. We have omitted RAG and tool use,
which means the "grounding" wouldn't make sense when run out-the-box, but tool use or
RAG could be added to the agent easily.

```python
client = unify.Unify("gpt-4o@openai")
evaluators = {
    "safe": HumanEvaluator(Safe),
    "inquires": HumanEvaluator(Inquires),
    "answers": HumanEvaluator(Answers),
    "grounds": HumanEvaluator(Grounds)
}

for datum in dataset:
    response = client.generate(**datum.prompt.dict())
    for evaluator in evaluators.values():
        evaluation = evaluator.evaluate(
            prompt=datum.prompt,
            response=response,
            agent=client
        )
        print(evaluation)
```

If we wanted to get more details about **why** each physician chose the score they did,
then they could make use of the extra `rationale` argument, which Unify supports for
all evaluations. The `_evaluate` method of `HumanEvaluator` could simply be extended as
follows (only showing the *added* lines at the bottom of the function):

```python
    ...
    rationale = input("What is your reasoning for giving this score? "
                      "Be as descriptive as you can be:")
    return self.class_config(response), rationale
```

The `rationale` will then be stored in the `Evaluation` instances which are returned
from `evaluator.evaluate`, and will be printed in the evaluation loop shown above
any code changes.

### Code Evaluator

Let's look at an example focused on code generation. In this example,
we will also make use of the extra fields in the `Dataset`, in order to store some
reference inputs and correct answers for each function implemented by the LLM:

```python
import random
import unify
from typing import Union, Type, Dict
system_msg = "You are an expert software engineer, " \
             "write the code asked of you to the highest quality. Give good variable " \
             "names, ensure the code compiles and is robust to edge cases, and always "\
             "gives the correct result."
questions = [
    "Write a python function to sort and merge two lists.",
    "Write a python function to find the nth largest number in a set.",
    "Write a python function to remove all None values from a dictionary."
]
inputs = [
    [([random.random() for _ in range(10)],
      [random.random() for _ in range(10)]) for _ in range(3)],
    [set([random.random() for _ in range(10)]) for _ in range(3)],
    [{"a": 1., "b": None, "c": 3.}, {"a": 1., "b": 2., "c": 3.}, {"a": None, "b": 2.}],
]
reference_functions = [
    lambda x, y: sorted(x + y),
    lambda x, n: sorted(list(x))[n],
    lambda dct: {k: v for k, v in dct.items() if v is not None}
]
answers = [fn(i) for i, fn in zip(inputs, reference_functions)]
prompts = [unify.Prompt(q, system_message=system_msg) for q in questions]
data = [unify.Datum(prompt=p, inputs=i, answers=a)
        for p, i, a in zip(prompts, inputs, answers)]
dataset = unify.Dataset(data)
```

We will create two evaluators, one which checks whether the code actually *runs* without
raising an error (irrespective of whether or not the answer is correct) and another
which determines whether or not the answer was correct. The scores for these two
evaluators can be implemented as follows:

```python
class Runs(unify.Score):

    @property
    def config(self) -> Dict[float, str]:
        return {
            0.: "An error is raised when the code is run.",
            1.: "Code runs without error."
        }
```
```python
class Correct(unify.Score):

    @property
    def config(self) -> Dict[float, str]:
        return {
            0.: "The answer was incorrect.",
            1.: "The answer was correct."
        }
```

The first evaluator can be implemented as follows, by making use of the extra
input data stored in the dataset:

```python
class RunsEvaluator(unify.Evaluator):

    @property
    def class_config(self) -> Type[Runs]:
        return Runs

    def _evaluate(self, prompt: str, response: str, inputs: List[Any]) -> bool:
        fn = eval(response)
        for inp in inputs:
            try:
                fn(inp)
                return True
            except:
                return False
```

The second evaluator can be implemented as follows, by also making use of the extra
answers stored in the dataset:

```python
class CorrectEvaluator(unify.Evaluator):

    @property
    def class_config(self) -> Type[Correct]:
        return Correct

    def _evaluate(self, prompt: str, response: str, inputs: List[Any], answers: List[Any]) -> bool:
        fn = eval(response)
        for inp, ans in zip(inputs, answers):
            try:
                llm_answer = fn(inp)
                return True
            except:
                return False
```

## Agents

## LLM Judges

## LLM Juries

## Evaluating the Evaluators

## Uploading

## Downloading

## Synchronizing
